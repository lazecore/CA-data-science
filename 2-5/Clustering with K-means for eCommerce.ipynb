{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with K-means for eCommerce\n",
    "## [RFM Analysis](https://medium.com/mlearning-ai/crm-analytics-customer-segmentation-customer-lifetime-value-prediction-1163fa6e4ae9)\n",
    "Understand customers' buying patterns using 3 'customer lifetime value' metrics: \n",
    "- Recency (how many days ago was their last purchase?)\n",
    "- Frequency (how many times did they purchase?) \n",
    "- Monetary (how much money did they spend?)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import modules and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, metrics, cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "dataset = pd.read_csv('data.csv', header = 0 , encoding = 'unicode_escape')\n",
    "\n",
    "df = dataset.copy() #backup copy!\n",
    "\n",
    "df.head()\n",
    "\n",
    "# transform into a df like this: | Customer ID | Recency | Frequency | Monetary |\n",
    "today_date = dt.datetime(2011,12,11)\n",
    "#TODO: change to .today day\n",
    "\n",
    "rfm = df.groupby('CustomerID').agg({'InvoiceDate': lambda invoice_date: (today_date - invoice_date.max()).days,\n",
    "                                    'InvoiceNo': lambda invoice: invoice.nunique(),\n",
    "                                    'TotalPrice': lambda total_price: total_price.sum()})\n",
    "\n",
    "rfm.columns = ['recency','frequency','monetary']\n",
    "rfm = rfm[(rfm['monetary'] > 0)]\n",
    "rfm = rfm.reset_index()\n",
    "rfm.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean/preprocess data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View descriptive statistics and compare to model requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe\n",
    "print(dataframe.dtypes)\n",
    "print(dataframe.isnull().sum())\n",
    "#distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that:\n",
    "\n",
    "- There are XXX variables with missing values.\n",
    "- There are XXX negative values\n",
    "- The dtypes are XXcorrect\n",
    "- The distribution is XXX\n",
    "\n",
    "KMeans requires:\n",
    "- XXX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use IQR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling: Z-Score Standardisation\n",
    "Standardisation allows us to compare features' values on a similar scale. This is done typically so that mean of all data types becomes 0, and the scale is based on the unit variance from this new mean. \n",
    "\n",
    "StandardScaler() requires a normal gaussian distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "\n",
    "print('Missing Values: {}'.format(df.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaling the attributes\n",
    "rfm_df = rfm[['Expenditure', 'Frequency', 'Recency']]\n",
    "\n",
    "# Instantiate\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit_transform\n",
    "rfm_df_scaled = pd.DataFrame(scaler.fit_transform(rfm_df)) #change back if doesn't work\n",
    "rfm_df_scaled.columns = ['Expenditure', 'Frequency', 'Recency']\n",
    "rfm_df_scaled.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Later: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pick out better scalers here? \n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal component analysis (PCA)\n",
    "if using more dimensions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### more steps: \n",
    "polynomial (preprocessing) https://scikit-learn.org/stable/modules/preprocessing.html#polynomial-features\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialise K-Means model and clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, max_iter=50)\n",
    "#TODO: add k-means++ param - maybe later in optimisation phase\n",
    "\n",
    "kmeans.fit(rfm_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assign the labels to each data point, and execute the following script.\n",
    "kmeans.labels_\n",
    "label_list=kmeans.labels_\n",
    "sorted(Counter(label_list).items())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimise the number of clusters (k)\n",
    "K-means is sensitive not only to the placement but also to the number of clusters. The Elbow Method and Silhouette Score can help us find the best number to fit the data well but without overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_data = rfm.loc[:,['recency_score','frequency_score']]\n",
    "\n",
    "inertia = []\n",
    "\n",
    "k = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "for i in k:\n",
    "    \n",
    "    kmean = KMeans(n_clusters = i)\n",
    "    kmean.fit(kmeans_data)\n",
    "    inertia.append(kmean.inertia_)\n",
    "    \n",
    "data = go.Scatter(x = k, y = inertia, mode = 'lines + markers', marker = dict(size= 10))\n",
    "\n",
    "layout = go.Layout(title = {'text' : 'Elbow Method',\n",
    "                            'y' : 0.9,\n",
    "                            'x' : 0.5,\n",
    "                            'xanchor' : 'center',\n",
    "                            'yanchor' : 'top'},\n",
    "                   width = 650,\n",
    "                   height = 470,\n",
    "                   xaxis = dict(title = 'Number Of Clusters'),\n",
    "                   yaxis = dict(title = 'Sum of Squared Distance'),\n",
    "                   template = 'plotly_white')\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "iplot(fig)                            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 3, random_state = 42)\n",
    "kmeans.fit(kmeans_data)\n",
    "print('Silhoutte Score : {}'.format(round(metrics.silhouette_score(kmeans_data, kmeans.labels_), 3))) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare performance of base model to model using `k-means++` \n",
    "\n",
    "K-means is sensitive not only to the number of clusters but also their initial placement. We already accounted for this by repeating the clustering with new random starting points each time. Scikit learn also provides a method to try to deal with this problem. Setting the k-means++ hyperparameter in scikit-learn will initialise centroids that are spread-out rather than random. \n",
    "\n",
    "As the silhouette score measures the separation between clusters, we can measure the impact of the method's attempt to spread the initial centroids."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run k-optimised K-Means model and visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot to visualize Cluster Id vs Amount\n",
    "sns.boxplot(x='Cluster_Id', y='Amount', data=rfm)\n",
    "\n",
    "# Box plot to visualize Cluster Id vs Frequency\n",
    "sns.boxplot(x='Cluster_Id', y='Frequency', data=rfm)\n",
    "\n",
    "# Box plot to visualize Cluster Id vs Recency\n",
    "sns.boxplot(x='Cluster_Id', y='Recency', data=rfm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last: also try K-Medoids\n",
    "https://medium.com/@ali.soleymani.co/beyond-scikit-learn-is-it-time-to-retire-k-means-and-use-this-method-instead-b8eb9ca9079a"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
