{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlueBerry Winery - Wine Quality Prediction with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop models to determine the quality of the wines produced based on their chemical composition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>4893</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>4894</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>4895</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>4896</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>4897</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  fixed acidity  volatile acidity  citric acid  \\\n",
       "0              0            7.4              0.70         0.00   \n",
       "1              1            7.8              0.88         0.00   \n",
       "2              2            7.8              0.76         0.04   \n",
       "3              3           11.2              0.28         0.56   \n",
       "4              4            7.4              0.70         0.00   \n",
       "...          ...            ...               ...          ...   \n",
       "6492        4893            6.2              0.21         0.29   \n",
       "6493        4894            6.6              0.32         0.36   \n",
       "6494        4895            6.5              0.24         0.19   \n",
       "6495        4896            5.5              0.29         0.30   \n",
       "6496        4897            6.0              0.21         0.38   \n",
       "\n",
       "      residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  \\\n",
       "0                1.9      0.076                 11.0                  34.0   \n",
       "1                2.6      0.098                 25.0                  67.0   \n",
       "2                2.3      0.092                 15.0                  54.0   \n",
       "3                1.9      0.075                 17.0                  60.0   \n",
       "4                1.9      0.076                 11.0                  34.0   \n",
       "...              ...        ...                  ...                   ...   \n",
       "6492             1.6      0.039                 24.0                  92.0   \n",
       "6493             8.0      0.047                 57.0                 168.0   \n",
       "6494             1.2      0.041                 30.0                 111.0   \n",
       "6495             1.1      0.022                 20.0                 110.0   \n",
       "6496             0.8      0.020                 22.0                  98.0   \n",
       "\n",
       "      density    pH  sulphates  alcohol  quality quality_label  color  \n",
       "0     0.99780  3.51       0.56      9.4        5           low    red  \n",
       "1     0.99680  3.20       0.68      9.8        5           low    red  \n",
       "2     0.99700  3.26       0.65      9.8        5           low    red  \n",
       "3     0.99800  3.16       0.58      9.8        6        medium    red  \n",
       "4     0.99780  3.51       0.56      9.4        5           low    red  \n",
       "...       ...   ...        ...      ...      ...           ...    ...  \n",
       "6492  0.99114  3.27       0.50     11.2        6        medium  white  \n",
       "6493  0.99490  3.15       0.46      9.6        5           low  white  \n",
       "6494  0.99254  2.99       0.46      9.4        6        medium  white  \n",
       "6495  0.98869  3.34       0.38     12.8        7        medium  white  \n",
       "6496  0.98941  3.26       0.32     11.8        6        medium  white  \n",
       "\n",
       "[6497 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "red_wine = pd.read_csv('winequality-red.csv', sep=';')\n",
    "white_wine = pd.read_csv('winequality-white.csv', sep=';')\n",
    "wines = pd.read_csv('wines_combined.csv')\n",
    "display(wines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Change column headings to snake_case and use IUPAC standard spelling ('sulfates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>4893</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>4894</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>4895</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>4896</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>4897</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  fixed acidity  volatile acidity  citric acid  \\\n",
       "0              0            7.4              0.70         0.00   \n",
       "1              1            7.8              0.88         0.00   \n",
       "2              2            7.8              0.76         0.04   \n",
       "3              3           11.2              0.28         0.56   \n",
       "4              4            7.4              0.70         0.00   \n",
       "...          ...            ...               ...          ...   \n",
       "6492        4893            6.2              0.21         0.29   \n",
       "6493        4894            6.6              0.32         0.36   \n",
       "6494        4895            6.5              0.24         0.19   \n",
       "6495        4896            5.5              0.29         0.30   \n",
       "6496        4897            6.0              0.21         0.38   \n",
       "\n",
       "      residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  \\\n",
       "0                1.9      0.076                 11.0                  34.0   \n",
       "1                2.6      0.098                 25.0                  67.0   \n",
       "2                2.3      0.092                 15.0                  54.0   \n",
       "3                1.9      0.075                 17.0                  60.0   \n",
       "4                1.9      0.076                 11.0                  34.0   \n",
       "...              ...        ...                  ...                   ...   \n",
       "6492             1.6      0.039                 24.0                  92.0   \n",
       "6493             8.0      0.047                 57.0                 168.0   \n",
       "6494             1.2      0.041                 30.0                 111.0   \n",
       "6495             1.1      0.022                 20.0                 110.0   \n",
       "6496             0.8      0.020                 22.0                  98.0   \n",
       "\n",
       "      density    pH  sulphates  alcohol  quality quality_label  color  \n",
       "0     0.99780  3.51       0.56      9.4        5           low    red  \n",
       "1     0.99680  3.20       0.68      9.8        5           low    red  \n",
       "2     0.99700  3.26       0.65      9.8        5           low    red  \n",
       "3     0.99800  3.16       0.58      9.8        6        medium    red  \n",
       "4     0.99780  3.51       0.56      9.4        5           low    red  \n",
       "...       ...   ...        ...      ...      ...           ...    ...  \n",
       "6492  0.99114  3.27       0.50     11.2        6        medium  white  \n",
       "6493  0.99490  3.15       0.46      9.6        5           low  white  \n",
       "6494  0.99254  2.99       0.46      9.4        6        medium  white  \n",
       "6495  0.98869  3.34       0.38     12.8        7        medium  white  \n",
       "6496  0.98941  3.26       0.32     11.8        6        medium  white  \n",
       "\n",
       "[6497 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>4893</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>4894</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>4895</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>4896</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>4897</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  fixed_acidity  volatile_acidity  citric_acid  \\\n",
       "0              0            7.4              0.70         0.00   \n",
       "1              1            7.8              0.88         0.00   \n",
       "2              2            7.8              0.76         0.04   \n",
       "3              3           11.2              0.28         0.56   \n",
       "4              4            7.4              0.70         0.00   \n",
       "...          ...            ...               ...          ...   \n",
       "6492        4893            6.2              0.21         0.29   \n",
       "6493        4894            6.6              0.32         0.36   \n",
       "6494        4895            6.5              0.24         0.19   \n",
       "6495        4896            5.5              0.29         0.30   \n",
       "6496        4897            6.0              0.21         0.38   \n",
       "\n",
       "      residual_sugar  chlorides  free_sulfur_dioxide  total_sulfur_dioxide  \\\n",
       "0                1.9      0.076                 11.0                  34.0   \n",
       "1                2.6      0.098                 25.0                  67.0   \n",
       "2                2.3      0.092                 15.0                  54.0   \n",
       "3                1.9      0.075                 17.0                  60.0   \n",
       "4                1.9      0.076                 11.0                  34.0   \n",
       "...              ...        ...                  ...                   ...   \n",
       "6492             1.6      0.039                 24.0                  92.0   \n",
       "6493             8.0      0.047                 57.0                 168.0   \n",
       "6494             1.2      0.041                 30.0                 111.0   \n",
       "6495             1.1      0.022                 20.0                 110.0   \n",
       "6496             0.8      0.020                 22.0                  98.0   \n",
       "\n",
       "      density    pH  sulfates  alcohol  quality quality_label  color  \n",
       "0     0.99780  3.51      0.56      9.4        5           low    red  \n",
       "1     0.99680  3.20      0.68      9.8        5           low    red  \n",
       "2     0.99700  3.26      0.65      9.8        5           low    red  \n",
       "3     0.99800  3.16      0.58      9.8        6        medium    red  \n",
       "4     0.99780  3.51      0.56      9.4        5           low    red  \n",
       "...       ...   ...       ...      ...      ...           ...    ...  \n",
       "6492  0.99114  3.27      0.50     11.2        6        medium  white  \n",
       "6493  0.99490  3.15      0.46      9.6        5           low  white  \n",
       "6494  0.99254  2.99      0.46      9.4        6        medium  white  \n",
       "6495  0.98869  3.34      0.38     12.8        7        medium  white  \n",
       "6496  0.98941  3.26      0.32     11.8        6        medium  white  \n",
       "\n",
       "[6497 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(wines)\n",
    "wines = wines.rename(columns={\"fixed acidity\": \"fixed_acidity\", \"volatile acidity\": \"volatile_acidity\", \"citric acid\": \"citric_acid\", \"residual sugar\" : \"residual_sugar\", \"free sulfur dioxide\" : \"free_sulfur_dioxide\", \"total sulfur dioxide\" : \"total_sulfur_dioxide\", \"sulphates\" : \"sulfates\"})\n",
    "display(wines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Drop old index values form red/white datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines = wines.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0               7.4              0.70         0.00             1.9      0.076   \n",
       "1               7.8              0.88         0.00             2.6      0.098   \n",
       "2               7.8              0.76         0.04             2.3      0.092   \n",
       "3              11.2              0.28         0.56             1.9      0.075   \n",
       "4               7.4              0.70         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "6492            6.2              0.21         0.29             1.6      0.039   \n",
       "6493            6.6              0.32         0.36             8.0      0.047   \n",
       "6494            6.5              0.24         0.19             1.2      0.041   \n",
       "6495            5.5              0.29         0.30             1.1      0.022   \n",
       "6496            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulfates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51      0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20      0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26      0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16      0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51      0.56   \n",
       "...                   ...                   ...      ...   ...       ...   \n",
       "6492                 24.0                  92.0  0.99114  3.27      0.50   \n",
       "6493                 57.0                 168.0  0.99490  3.15      0.46   \n",
       "6494                 30.0                 111.0  0.99254  2.99      0.46   \n",
       "6495                 20.0                 110.0  0.98869  3.34      0.38   \n",
       "6496                 22.0                  98.0  0.98941  3.26      0.32   \n",
       "\n",
       "      alcohol  quality quality_label  color  \n",
       "0         9.4        5           low    red  \n",
       "1         9.8        5           low    red  \n",
       "2         9.8        5           low    red  \n",
       "3         9.8        6        medium    red  \n",
       "4         9.4        5           low    red  \n",
       "...       ...      ...           ...    ...  \n",
       "6492     11.2        6        medium  white  \n",
       "6493      9.6        5           low  white  \n",
       "6494      9.4        6        medium  white  \n",
       "6495     12.8        7        medium  white  \n",
       "6496     11.8        6        medium  white  \n",
       "\n",
       "[6497 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(wines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Counts of duplicated items:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False    5320\n",
       "True     1177\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>6492</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>6493</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>6494</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>6495</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>6496</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5320 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  fixed_acidity  volatile_acidity  citric_acid  residual_sugar  \\\n",
       "0         1            7.8              0.88         0.00             2.6   \n",
       "1         2            7.8              0.76         0.04             2.3   \n",
       "2         3           11.2              0.28         0.56             1.9   \n",
       "3         4            7.4              0.70         0.00             1.9   \n",
       "4         5            7.4              0.66         0.00             1.8   \n",
       "...     ...            ...               ...          ...             ...   \n",
       "5315   6492            6.2              0.21         0.29             1.6   \n",
       "5316   6493            6.6              0.32         0.36             8.0   \n",
       "5317   6494            6.5              0.24         0.19             1.2   \n",
       "5318   6495            5.5              0.29         0.30             1.1   \n",
       "5319   6496            6.0              0.21         0.38             0.8   \n",
       "\n",
       "      chlorides  free_sulfur_dioxide  total_sulfur_dioxide  density    pH  \\\n",
       "0         0.098                 25.0                  67.0  0.99680  3.20   \n",
       "1         0.092                 15.0                  54.0  0.99700  3.26   \n",
       "2         0.075                 17.0                  60.0  0.99800  3.16   \n",
       "3         0.076                 11.0                  34.0  0.99780  3.51   \n",
       "4         0.075                 13.0                  40.0  0.99780  3.51   \n",
       "...         ...                  ...                   ...      ...   ...   \n",
       "5315      0.039                 24.0                  92.0  0.99114  3.27   \n",
       "5316      0.047                 57.0                 168.0  0.99490  3.15   \n",
       "5317      0.041                 30.0                 111.0  0.99254  2.99   \n",
       "5318      0.022                 20.0                 110.0  0.98869  3.34   \n",
       "5319      0.020                 22.0                  98.0  0.98941  3.26   \n",
       "\n",
       "      sulfates  alcohol  quality quality_label  color  \n",
       "0         0.68      9.8        5           low    red  \n",
       "1         0.65      9.8        5           low    red  \n",
       "2         0.58      9.8        6        medium    red  \n",
       "3         0.56      9.4        5           low    red  \n",
       "4         0.56      9.4        5           low    red  \n",
       "...        ...      ...      ...           ...    ...  \n",
       "5315      0.50     11.2        6        medium  white  \n",
       "5316      0.46      9.6        5           low  white  \n",
       "5317      0.46      9.4        6        medium  white  \n",
       "5318      0.38     12.8        7        medium  white  \n",
       "5319      0.32     11.8        6        medium  white  \n",
       "\n",
       "[5320 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, check for duplicates using duplicated() ## FIXED now that second index removed\n",
    "# use the value counts of duplicated() to show how many items are duplicated\n",
    "display('Counts of duplicated items:', wines.duplicated().value_counts())\n",
    "\n",
    "#drop dupliactes and reset index\n",
    "wines_clean = wines.drop_duplicates(keep='last').reset_index()\n",
    "display(wines_clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Drop wines outside legal limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_sulfur_dioxide_max =  400  ## including ‘colheita tardia’ https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32019R0934&rid=2\n",
    "## decided not to do this as only one wine falls into this, if indeed sweeter, late-harvest wines are included"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check and change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                     int64\n",
       "fixed_acidity           float64\n",
       "volatile_acidity        float64\n",
       "citric_acid             float64\n",
       "residual_sugar          float64\n",
       "chlorides               float64\n",
       "free_sulfur_dioxide     float64\n",
       "total_sulfur_dioxide    float64\n",
       "density                 float64\n",
       "pH                      float64\n",
       "sulfates                float64\n",
       "alcohol                 float64\n",
       "quality                   int64\n",
       "quality_label            object\n",
       "color                    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wines_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                      int64\n",
       "fixed_acidity            float64\n",
       "volatile_acidity         float64\n",
       "citric_acid              float64\n",
       "residual_sugar           float64\n",
       "chlorides                float64\n",
       "free_sulfur_dioxide      float64\n",
       "total_sulfur_dioxide     float64\n",
       "density                  float64\n",
       "pH                       float64\n",
       "sulfates                 float64\n",
       "alcohol                  float64\n",
       "quality                    int64\n",
       "quality_label           category\n",
       "color                   category\n",
       "dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast columns to Categorical (pandas) data type:\n",
    "# color, unordered\n",
    "wines_clean['color'] = pd.Categorical(wines_clean['color'],\n",
    "ordered=False)\n",
    "\n",
    "# quality_label, ordered low to high\n",
    "wines_clean['quality_label'] = pd.Categorical(wines_clean['quality_label'],\n",
    "categories=['low', 'medium', 'high'], ordered=True)\n",
    "\n",
    "wines_clean.dtypes\n",
    "\n",
    "#display(wines_clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate skewness and kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index                   0.050375\n",
      "fixed_acidity           1.650417\n",
      "volatile_acidity        1.504557\n",
      "citric_acid             0.484309\n",
      "residual_sugar          1.706550\n",
      "chlorides               5.338237\n",
      "free_sulfur_dioxide     1.362719\n",
      "total_sulfur_dioxide    0.063614\n",
      "density                 0.666326\n",
      "pH                      0.389969\n",
      "sulfates                1.809454\n",
      "alcohol                 0.545696\n",
      "quality                 0.147467\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print((wines_clean.select_dtypes(include=['int64', 'float64'])).astype(float).skew())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness: Normal distribution = 0 skewness. Between -0.5 and 0.5 is considered an approximately symmetric distribution. Here, most values falll outside of this.\n",
    "Values beyond −2 and +2 are considered indicative of substantial non-normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index                   -1.188777\n",
      "fixed_acidity            4.589079\n",
      "volatile_acidity         2.863175\n",
      "citric_acid              2.582471\n",
      "residual_sugar           7.025595\n",
      "chlorides               48.260708\n",
      "free_sulfur_dioxide      9.520706\n",
      "total_sulfur_dioxide    -0.299997\n",
      "density                  8.711498\n",
      "pH                       0.431811\n",
      "sulfates                 8.612917\n",
      "alcohol                 -0.538169\n",
      "quality                  0.298100\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print((wines_clean.select_dtypes(include=['int64', 'float64'])).astype(float).kurt())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurtosis: over 3 (or sometimes over 2) is considered high - many data items in the 'tails' of the distribution may be outliers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index                  -1.188777\n",
      "fixed_acidity           2.588816\n",
      "volatile_acidity        1.229545\n",
      "citric_acid             0.665174\n",
      "residual_sugar          0.575966\n",
      "chlorides               7.690028\n",
      "free_sulfur_dioxide    -0.340481\n",
      "total_sulfur_dioxide   -0.680072\n",
      "density                -0.874793\n",
      "pH                     -0.164159\n",
      "sulfates                1.185017\n",
      "alcohol                -0.675095\n",
      "quality                 0.298100\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#use numpy.clip to cap and floor relevant values to within 1 and 99 percentile (source: https://stackoverflow.com/questions/42207920/how-to-take-floor-and-capping-for-removing-outliers)\n",
    "for col in wines_clean.columns[1:12]:\n",
    "    percentiles = wines_clean[col].quantile([0.01, 0.99]).values\n",
    "    wines_clean[col] = np.clip(wines_clean[col], percentiles[0], percentiles[1])\n",
    "\n",
    "#wines_clean.describe()\n",
    "print((wines_clean.select_dtypes(include=['int64', 'float64'])).astype(float).kurt())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now most values are in an acceptable range except chlorides and fixed acidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index                  -1.188777\n",
      "fixed_acidity           0.164222\n",
      "volatile_acidity        1.229545\n",
      "citric_acid             0.665174\n",
      "residual_sugar          0.575966\n",
      "chlorides              -0.090249\n",
      "free_sulfur_dioxide    -0.340481\n",
      "total_sulfur_dioxide   -0.680072\n",
      "density                -0.874793\n",
      "pH                     -0.164159\n",
      "sulfates                1.185017\n",
      "alcohol                -0.675095\n",
      "quality                 0.298100\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for col in ['chlorides', 'fixed_acidity']:\n",
    "    percentiles = wines_clean[col].quantile([0.05, 0.95]).values\n",
    "    wines_clean[col] = np.clip(wines_clean[col], percentiles[0], percentiles[1])\n",
    "print((wines_clean.select_dtypes(include=['int64', 'float64'])).astype(float).kurt())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing for ML 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Encoding categorical variables\n",
    "\n",
    "Use scikit. Resources: \n",
    "- https://scikit-learn.org/stable/modules/preprocessing.html \n",
    "- https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n",
    "- https://scikit-learn.org/stable/modules/preprocessing_targets.html#preprocessing-targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Label (Ordinal: Label Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "      <th>quality_label_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.996800</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>6492</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.991140</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>6493</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>6494</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.992540</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>6495</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.028</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.988892</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>6496</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.028</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.989410</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5320 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  fixed_acidity  volatile_acidity  citric_acid  residual_sugar  \\\n",
       "0         1            7.8              0.88         0.00             2.6   \n",
       "1         2            7.8              0.76         0.04             2.3   \n",
       "2         3            9.8              0.28         0.56             1.9   \n",
       "3         4            7.4              0.70         0.00             1.9   \n",
       "4         5            7.4              0.66         0.00             1.8   \n",
       "...     ...            ...               ...          ...             ...   \n",
       "5315   6492            6.2              0.21         0.29             1.6   \n",
       "5316   6493            6.6              0.32         0.36             8.0   \n",
       "5317   6494            6.5              0.24         0.19             1.2   \n",
       "5318   6495            5.6              0.29         0.30             1.1   \n",
       "5319   6496            6.0              0.21         0.38             0.9   \n",
       "\n",
       "      chlorides  free_sulfur_dioxide  total_sulfur_dioxide   density    pH  \\\n",
       "0         0.098                 25.0                  67.0  0.996800  3.20   \n",
       "1         0.092                 15.0                  54.0  0.997000  3.26   \n",
       "2         0.075                 17.0                  60.0  0.998000  3.16   \n",
       "3         0.076                 11.0                  34.0  0.997800  3.51   \n",
       "4         0.075                 13.0                  40.0  0.997800  3.51   \n",
       "...         ...                  ...                   ...       ...   ...   \n",
       "5315      0.039                 24.0                  92.0  0.991140  3.27   \n",
       "5316      0.047                 57.0                 168.0  0.994900  3.15   \n",
       "5317      0.041                 30.0                 111.0  0.992540  2.99   \n",
       "5318      0.028                 20.0                 110.0  0.988892  3.34   \n",
       "5319      0.028                 22.0                  98.0  0.989410  3.26   \n",
       "\n",
       "      sulfates  alcohol  quality quality_label  color  quality_label_cat  \n",
       "0         0.68      9.8        5           low    red                  0  \n",
       "1         0.65      9.8        5           low    red                  0  \n",
       "2         0.58      9.8        6        medium    red                  1  \n",
       "3         0.56      9.4        5           low    red                  0  \n",
       "4         0.56      9.4        5           low    red                  0  \n",
       "...        ...      ...      ...           ...    ...                ...  \n",
       "5315      0.50     11.2        6        medium  white                  1  \n",
       "5316      0.46      9.6        5           low  white                  0  \n",
       "5317      0.46      9.4        6        medium  white                  1  \n",
       "5318      0.38     12.8        7        medium  white                  1  \n",
       "5319      0.32     11.8        6        medium  white                  1  \n",
       "\n",
       "[5320 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add a new cat_ variable storing a numerical code for each category\n",
    "## note: didn't use LabelEncoder, used .cat.codes as per https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n",
    "\n",
    "wines_clean['quality_label_cat'] = wines_clean['quality_label'].cat.codes \n",
    "display(wines_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3179\n",
       "0    1988\n",
       "2     153\n",
       "Name: quality_label_cat, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "medium    3179\n",
       "low       1988\n",
       "high       153\n",
       "Name: quality_label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(wines_clean.quality_label_cat.value_counts(), wines_clean.quality_label.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color (Nominal: One-Hot Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "      <th>quality_label_cat</th>\n",
       "      <th>color_cat</th>\n",
       "      <th>is_red</th>\n",
       "      <th>is_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.996800</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>6492</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.991140</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>6493</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>6494</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.992540</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>6495</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.028</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.988892</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>6496</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.028</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.989410</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5320 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  fixed_acidity  volatile_acidity  citric_acid  residual_sugar  \\\n",
       "0         1            7.8              0.88         0.00             2.6   \n",
       "1         2            7.8              0.76         0.04             2.3   \n",
       "2         3            9.8              0.28         0.56             1.9   \n",
       "3         4            7.4              0.70         0.00             1.9   \n",
       "4         5            7.4              0.66         0.00             1.8   \n",
       "...     ...            ...               ...          ...             ...   \n",
       "5315   6492            6.2              0.21         0.29             1.6   \n",
       "5316   6493            6.6              0.32         0.36             8.0   \n",
       "5317   6494            6.5              0.24         0.19             1.2   \n",
       "5318   6495            5.6              0.29         0.30             1.1   \n",
       "5319   6496            6.0              0.21         0.38             0.9   \n",
       "\n",
       "      chlorides  free_sulfur_dioxide  total_sulfur_dioxide   density    pH  \\\n",
       "0         0.098                 25.0                  67.0  0.996800  3.20   \n",
       "1         0.092                 15.0                  54.0  0.997000  3.26   \n",
       "2         0.075                 17.0                  60.0  0.998000  3.16   \n",
       "3         0.076                 11.0                  34.0  0.997800  3.51   \n",
       "4         0.075                 13.0                  40.0  0.997800  3.51   \n",
       "...         ...                  ...                   ...       ...   ...   \n",
       "5315      0.039                 24.0                  92.0  0.991140  3.27   \n",
       "5316      0.047                 57.0                 168.0  0.994900  3.15   \n",
       "5317      0.041                 30.0                 111.0  0.992540  2.99   \n",
       "5318      0.028                 20.0                 110.0  0.988892  3.34   \n",
       "5319      0.028                 22.0                  98.0  0.989410  3.26   \n",
       "\n",
       "      sulfates  alcohol  quality quality_label  color  quality_label_cat  \\\n",
       "0         0.68      9.8        5           low    red                  0   \n",
       "1         0.65      9.8        5           low    red                  0   \n",
       "2         0.58      9.8        6        medium    red                  1   \n",
       "3         0.56      9.4        5           low    red                  0   \n",
       "4         0.56      9.4        5           low    red                  0   \n",
       "...        ...      ...      ...           ...    ...                ...   \n",
       "5315      0.50     11.2        6        medium  white                  1   \n",
       "5316      0.46      9.6        5           low  white                  0   \n",
       "5317      0.46      9.4        6        medium  white                  1   \n",
       "5318      0.38     12.8        7        medium  white                  1   \n",
       "5319      0.32     11.8        6        medium  white                  1   \n",
       "\n",
       "      color_cat  is_red  is_white  \n",
       "0             0     1.0       0.0  \n",
       "1             0     1.0       0.0  \n",
       "2             0     1.0       0.0  \n",
       "3             0     1.0       0.0  \n",
       "4             0     1.0       0.0  \n",
       "...         ...     ...       ...  \n",
       "5315          1     0.0       1.0  \n",
       "5316          1     0.0       1.0  \n",
       "5317          1     0.0       1.0  \n",
       "5318          1     0.0       1.0  \n",
       "5319          1     0.0       1.0  \n",
       "\n",
       "[5320 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OHE creates new dummy variable columns for each category, with binary encoding (one of them 1, all others 0) indicating whether the category is applied\n",
    "\n",
    "# Step 1: OHE needs numerica data, so first transform categorical data using LabelEncoder() [This time really use LabelEncoder!]\n",
    "\n",
    "# create an instance of labelencoder  -TODO: explain this more\n",
    "labelencoder = LabelEncoder()\n",
    "# make new column and apply numerical category values\n",
    "wines_clean['color_cat'] = labelencoder.fit_transform(wines_clean['color'])\n",
    "\n",
    "# Step 2: Now use OneHotEncoder. reference: https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n",
    "## First, create an instance of one-hot-encoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# pass in color_cat to make a df containing the multiple (here, 2) binary encoding columns\n",
    "enc_df = pd.DataFrame(enc.fit_transform(wines_clean[['color_cat']]).toarray())\n",
    "#display('encoder array:', enc_df)\n",
    "\n",
    "# merge df with wines_clean\n",
    "wines_clean = wines_clean.join(enc_df)\n",
    "\n",
    "# rename 0 and 1 (this time manually but look into get_feature_names_out and ColumnTransformer)\n",
    "wines_clean = wines_clean.rename(columns={0: 'is_red', 1: 'is_white'})\n",
    "\n",
    "#drop extra index\n",
    "#wines_clean = wines_clean.drop('index', axis=1)\n",
    "display(wines_clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Linear Regression Machine Learning Model with Single Variable: Density)\n",
    "\n",
    "\n",
    "note: not to be confused with logistical regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Separate off training set (split)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `test_train_split` to separate testing and traning portions of the data. [criteria for % proportion, source]\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99584],\n",
       "       [0.997  ],\n",
       "       [0.99212],\n",
       "       ...,\n",
       "       [0.9939 ],\n",
       "       [0.99415],\n",
       "       [0.9938 ]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## first split for single variable linear regression\n",
    "#make new variables for???\n",
    "density = wines_clean.density\n",
    "quality = wines_clean.quality\n",
    "\n",
    "#first reshape (which data?? workbook says main data but web says test data) into ndarray otherwise we get an error #TODO: expand (https://stackoverflow.com/questions/47761744/cant-do-linear-regression-in-scikit-learn-due-to-reshaping-issue)\n",
    "density = density.values.reshape(-1, 1)\n",
    "quality = quality.values.reshape(-1, 1)\n",
    "\n",
    "#split data\n",
    "density_train, density_test, quality_train, quality_test = train_test_split(density, quality, test_size=0.2, random_state=0)\n",
    "density_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_density = linear_model.LinearRegression()\n",
    "reg_density.fit(density_train, quality_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make predicitons and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the ML model based on Living Area is: 12.49 %\n",
      "A wine of density 1 may have a quality of: [[5.23134774]] and is 12.49% accurate\n"
     ]
    }
   ],
   "source": [
    "# apply the .predict() method to make predictions using test data.\n",
    "#find accuracy score\n",
    "reg_density_score = (reg_density.score(density_test, quality_test) * 100).round(2)\n",
    "print(\"The accuracy of the ML model based on Living Area is:\", reg_density_score , \"%\")\n",
    "\n",
    "#pick a value to test prediction\n",
    "density_input = 1     ## variable to test y and X\n",
    "pred_1 = reg_density.predict([[density_input]]) \n",
    "print(f\"A wine of density {density_input} may have a quality of: {pred_1} and is {reg_density_score}% accurate\" )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.freecodecamp.org/news/how-to-build-and-train-linear-and-logistic-regression-ml-models-in-python/\n",
    "\n",
    "#define dfs for x and y\n",
    "y_data = wines_clean['quality_label_cat']\n",
    "x_data = wines_clean.drop(['index', 'quality_label_cat', 'color_cat', 'quality_label', 'quality', 'color'], axis = 1) \n",
    "\n",
    "#split training data\n",
    "x_training_data, x_test_data, y_training_data, y_test_data = train_test_split(x_data, y_data, test_size = 0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>is_red</th>\n",
       "      <th>is_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.996800</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.8</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.991140</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.992540</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>5.6</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.028</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.988892</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.028</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.989410</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5320 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0               7.8              0.88         0.00             2.6      0.098   \n",
       "1               7.8              0.76         0.04             2.3      0.092   \n",
       "2               9.8              0.28         0.56             1.9      0.075   \n",
       "3               7.4              0.70         0.00             1.9      0.076   \n",
       "4               7.4              0.66         0.00             1.8      0.075   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "5315            6.2              0.21         0.29             1.6      0.039   \n",
       "5316            6.6              0.32         0.36             8.0      0.047   \n",
       "5317            6.5              0.24         0.19             1.2      0.041   \n",
       "5318            5.6              0.29         0.30             1.1      0.028   \n",
       "5319            6.0              0.21         0.38             0.9      0.028   \n",
       "\n",
       "      free_sulfur_dioxide  total_sulfur_dioxide   density    pH  sulfates  \\\n",
       "0                    25.0                  67.0  0.996800  3.20      0.68   \n",
       "1                    15.0                  54.0  0.997000  3.26      0.65   \n",
       "2                    17.0                  60.0  0.998000  3.16      0.58   \n",
       "3                    11.0                  34.0  0.997800  3.51      0.56   \n",
       "4                    13.0                  40.0  0.997800  3.51      0.56   \n",
       "...                   ...                   ...       ...   ...       ...   \n",
       "5315                 24.0                  92.0  0.991140  3.27      0.50   \n",
       "5316                 57.0                 168.0  0.994900  3.15      0.46   \n",
       "5317                 30.0                 111.0  0.992540  2.99      0.46   \n",
       "5318                 20.0                 110.0  0.988892  3.34      0.38   \n",
       "5319                 22.0                  98.0  0.989410  3.26      0.32   \n",
       "\n",
       "      alcohol  is_red  is_white  \n",
       "0         9.8     1.0       0.0  \n",
       "1         9.8     1.0       0.0  \n",
       "2         9.8     1.0       0.0  \n",
       "3         9.4     1.0       0.0  \n",
       "4         9.4     1.0       0.0  \n",
       "...       ...     ...       ...  \n",
       "5315     11.2     0.0       1.0  \n",
       "5316      9.6     0.0       1.0  \n",
       "5317      9.4     0.0       1.0  \n",
       "5318     12.8     0.0       1.0  \n",
       "5319     11.8     0.0       1.0  \n",
       "\n",
       "[5320 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "5315    1\n",
       "5316    0\n",
       "5317    1\n",
       "5318    1\n",
       "5319    1\n",
       "Name: quality_label_cat, Length: 5320, dtype: int8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x_data)\n",
    "display(y_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=5000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=5000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=5000)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "#first, instantiate the model (create an instance of it with a name and any necessary parameters)\n",
    "lr = LogisticRegression(max_iter=5000)\n",
    "\n",
    "#then, use fit() to pass in the x and y training data\n",
    "lr.fit(x_training_data, y_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make predicitons and evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.62      0.64       383\n",
      "           1       0.75      0.81      0.78       651\n",
      "           2       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.72      1064\n",
      "   macro avg       0.47      0.48      0.47      1064\n",
      "weighted avg       0.70      0.72      0.71      1064\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions = lr.predict(x_test_data)\n",
    "print(classification_report(y_test_data, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Confusion Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification models. Returns a table of true/false - negative/positive predictions made by the model. It is not an evaluaiton metric but gives an overview. Not suitable for unbalanced data (which ours is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f623a396400>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test_data, predictions)).plot(cmap='BuPu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Accuracy Score (not useful with imbalanced classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7208646616541353"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_data, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Cohen-Kappa Score (better for imbalanced classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A measure of classifier performance compared to a random-guessing model, especially good for an imbalanced data set (0-1, higher is better)\n",
    "\n",
    "\n",
    "Resources:\n",
    "- https://analyticsindiamag.com/understanding-cohens-kappa-score-with-hands-on-implementation/\n",
    "- https://www.knime.com/blog/cohens-kappa-an-overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4103360253175895"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(y_test_data, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: Moderate agreement = 0.40 to 0.60"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple ML models in a function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import more modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score, precision_score, roc_auc_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "### from unsuccessful attempts to read exception (scikit warning) message to write to results ##\n",
    "import warnings\n",
    "import logging\n",
    "import traceback\n",
    "import sys\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function cycles through a list of models and runs them on x and y training and testing data inputs.\n",
    "- Logistic Regression (with various solvers)\n",
    "- Decision Tree Classifier\n",
    "- K-Nearest Neighbors Classifier (does not use a training set)\n",
    "- Support Vector Machine [LinearSVC()]\n",
    "- Naive Bayes [GaussianNB()]\n",
    "- Linear Discriminant Analysis\n",
    "- Random Forest\n",
    "- MLP Classifier\n",
    "\n",
    "It outputs the models' scores on various tests:\n",
    "- Accuracy Score - the percentage of predictions that were correct (not good for imbalanced data)\n",
    "- Cohen Kappa - \n",
    "- Precision - correct positives\n",
    "- F1 Score - score combining precision (correct positives) and recall (sensitivity)\n",
    "- maybe (roc_auc_score) #TODO\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warnings\n",
    "- Warnings are usually written to https://docs.python.org/3/library/sys.html#sys.stderr file objects/streams. \"To write or read binary data from/to the standard streams, use the underlying binary buffer object. For example, to write bytes to stdout, use sys.stdout.buffer.write(b'abc').\"\n",
    "\n",
    "- class warnings.catch_warnings(*, record=False, module=None, action=None, category=Warning, lineno=0, append=False)\n",
    "A context manager that copies and, upon exit, restores the warnings filter and the showwarning() function. \n",
    "If record is True, a list is returned that is progressively populated with objects as seen by a custom showwarning() function (which also suppresses output to sys.stdout). \n",
    "Each object in the list has attributes with the same names as the arguments to showwarning(). https://docs.python.org/3/library/warnings.html\n",
    "    - so maybe need to get length of a list and extract the -1th item. except, when i print there is no list.\n",
    "    - is it possible to use the 'action' do do this as well? lambda?\n",
    "\n",
    "- https://docs.python.org/3/library/logging.html\n",
    "- can possibly also use this but it uses with and assert and i don't know them https://docs.python.org/3/library/warnings.html#testing-warnings\n",
    "- https://docs.python.org/3/library/traceback.html and https://stackoverflow.com/questions/8238360/how-to-save-traceback-sys-exc-info-values-in-a-variable but uses try which i don't know and can't make work in a loop \n",
    "    -\"the problem with ipython or jupyter notebook env is that it has %tb magic which saves the traceback and makes it available at any point later. And as a result any locals() in all frames participating in the traceback will not be freed until the notebook exits or another exception will overwrite the previously stored backtrace. This is very problematic. It should not store the traceback w/o cleaning its frames. Fix submitted here.\" https://stackoverflow.com/questions/8238360/how-to-save-traceback-sys-exc-info-values-in-a-variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_warnings = pd.DataFrame()\n",
    "\n",
    "# lrlow = LogisticRegression(max_iter=50)\n",
    "\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.filterwarnings(category=ConvergenceWarning, action='always')\n",
    "#     try:\n",
    "#         lrlow.fit(x_data, y_data)\n",
    "#     except Warning:\n",
    "#         #df_warnings.append...\n",
    "#         print('Did not converge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.716165</td>\n",
       "      <td>0.398906</td>\n",
       "      <td>0.692765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.703271</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.429021</td>\n",
       "      <td>0.708343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.718194</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.718985</td>\n",
       "      <td>0.397935</td>\n",
       "      <td>0.694813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.615602</td>\n",
       "      <td>0.237768</td>\n",
       "      <td>0.624141</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.619636</td>\n",
       "      <td>0.109589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.636278</td>\n",
       "      <td>0.227523</td>\n",
       "      <td>0.612339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622866</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>0.545113</td>\n",
       "      <td>0.206846</td>\n",
       "      <td>0.665134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519474</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.601504</td>\n",
       "      <td>0.217438</td>\n",
       "      <td>0.615932</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.608152</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.412615</td>\n",
       "      <td>0.698718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709353</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.738722</td>\n",
       "      <td>0.449609</td>\n",
       "      <td>0.730086</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.727771</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.730263</td>\n",
       "      <td>0.407678</td>\n",
       "      <td>0.708516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.716165   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.732143   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.718985   \n",
       "3                           DecisionTreeClassifier()        0.615602   \n",
       "4                             KNeighborsClassifier()        0.636278   \n",
       "5                           LinearSVC(max_iter=3000)        0.545113   \n",
       "6                                       GaussianNB()        0.601504   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.721805   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.738722   \n",
       "9                                    MLPClassifier()        0.730263   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  \n",
       "0     0.398906   0.692765         0.000000  0.703271        0.000000  \n",
       "1     0.429021   0.708343         0.000000  0.718194        0.000000  \n",
       "2     0.397935   0.694813         0.000000  0.704100        0.000000  \n",
       "3     0.237768   0.624141         0.093023  0.619636        0.109589  \n",
       "4     0.227523   0.612339         0.000000  0.622866        0.000000  \n",
       "5     0.206846   0.665134         0.000000  0.519474        0.000000  \n",
       "6     0.217438   0.615932         0.103448  0.608152        0.136364  \n",
       "7     0.412615   0.698718         0.000000  0.709353        0.000000  \n",
       "8     0.449609   0.730086         0.500000  0.727771        0.062500  \n",
       "9     0.407678   0.708516         0.000000  0.710398        0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#instantiate all models with any parameters needed\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=3000)  #NB: 'lbfgs' is the default solver. it failed to converge, so trying various solvers\n",
    "lr1 = LogisticRegression(solver='liblinear', max_iter=3000)\n",
    "lr2 = LogisticRegression(solver='newton-cg', max_iter=3000)\n",
    "lr3 = LogisticRegression(solver='newton-cholesky', max_iter=3000)\n",
    "lr4 = LogisticRegression(solver='sag', max_iter=3000)   #NB: sag requires normalisation to work\n",
    "lr5 = LogisticRegression(solver='saga', max_iter=3000)  #NB:saga requires normalisation\n",
    "dt = DecisionTreeClassifier()\n",
    "kn = KNeighborsClassifier()\n",
    "ls = LinearSVC(dual = True, max_iter=3000)\n",
    "nb = GaussianNB()\n",
    "ld = LDA(n_components=1)\n",
    "rf = RandomForestClassifier()\n",
    "mp = MLPClassifier()\n",
    "#TODO: nn = sklearn.neural_network\n",
    "\n",
    "#create list of all model instances\n",
    "classifier_model_defs = [lr, lr1, lr5, dt, kn, ls, nb, ld, rf, mp] ## not used: lr2, lr3, lr4, nn\n",
    "\n",
    "#warnings.filterwarnings('always', category=ConvergenceWarning)\n",
    "\n",
    "#define function\n",
    "def deploy_models(x_training_data, x_test_data, y_training_data, y_test_data): #feed in all inputs needed by models here  \n",
    "    df_models_eval = pd.DataFrame(columns=['Model', 'Accuracy Score', 'Cohen Kappa', 'Precision', 'Precision: high', 'F1 Score', 'F1 Score: high']) # ,'Warning' #make a df to hold the scores\n",
    "    for model in classifier_model_defs:             #loop over models in list\n",
    "        model.fit(x_training_data, y_training_data) #use fit() to train each model on the x and y training data\n",
    "        pred = model.predict(x_test_data)           #predict, evaluate and output some things to compare...\n",
    "        acc = accuracy_score(y_test_data, pred)\n",
    "        kap = cohen_kappa_score(y_test_data, pred)\n",
    "        prc = precision_score(y_test_data, pred, average='weighted', zero_division=0)\n",
    "        prc2 = precision_score(y_test_data, pred, average='weighted', zero_division=0, labels=[2]) \n",
    "        f1 = f1_score(y_test_data, pred, average='weighted', zero_division=0) \n",
    "        f12 = f1_score(y_test_data, pred, labels=[2], average='weighted', zero_division=0)# , labels=[2] to just show high quality\n",
    "        #warn = 0 #sys.exc_info() #logging.LogRecord.getMessage() #traceback.extract_stack() #logging.Formatter(traceback.print_stack()) \n",
    "        #warnings.catch_warnings(,ConvergenceWarning) #sys.unraisablehook.err_msg #sys.stderr #logging.captureWarnings() \n",
    "        # #warnings.filterwarnings(action='ignore') \n",
    "        ## TODO: want to add error type so i can see which ones did not complete (ie Converge, so far)\n",
    "        #print(warn)\n",
    "        df_models_eval.loc[len(df_models_eval)] = [model, acc, kap, prc, prc2, f1, f12] #,warn #add row to the results df\n",
    "        #print('\\n', model, 'accuracy:', acc, 'kappa:', kap, 'f1:', f1) #test:print result per iteration (for when loop fails to finish and display df)\n",
    "    return(df_models_eval)\n",
    "\n",
    "#call function and save un-engineered test results to variable for comparison\n",
    "orig_results = deploy_models(x_training_data, x_test_data, y_training_data, y_test_data)\n",
    "\n",
    "display(orig_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test models also on their ability to predict color (type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.984023</td>\n",
       "      <td>0.957132</td>\n",
       "      <td>0.984155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.983890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.981203</td>\n",
       "      <td>0.949754</td>\n",
       "      <td>0.981198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.981084</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.903819</td>\n",
       "      <td>0.964136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.963919</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.970865</td>\n",
       "      <td>0.922598</td>\n",
       "      <td>0.970740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.970774</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.935150</td>\n",
       "      <td>0.822480</td>\n",
       "      <td>0.934511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.933892</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.928416</td>\n",
       "      <td>0.974218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.973268</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.978383</td>\n",
       "      <td>0.942854</td>\n",
       "      <td>0.978359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.978370</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.994361</td>\n",
       "      <td>0.985037</td>\n",
       "      <td>0.994370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.994347</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.995301</td>\n",
       "      <td>0.987516</td>\n",
       "      <td>0.995330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995286</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.984962</td>\n",
       "      <td>0.959804</td>\n",
       "      <td>0.985006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.984868</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.984023   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.981203   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.964286   \n",
       "3                           DecisionTreeClassifier()        0.970865   \n",
       "4                             KNeighborsClassifier()        0.935150   \n",
       "5                           LinearSVC(max_iter=3000)        0.973684   \n",
       "6                                       GaussianNB()        0.978383   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.994361   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.995301   \n",
       "9                                    MLPClassifier()        0.984962   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  \n",
       "0     0.957132   0.984155              0.0  0.983890             0.0  \n",
       "1     0.949754   0.981198              0.0  0.981084             0.0  \n",
       "2     0.903819   0.964136              0.0  0.963919             0.0  \n",
       "3     0.922598   0.970740              0.0  0.970774             0.0  \n",
       "4     0.822480   0.934511              0.0  0.933892             0.0  \n",
       "5     0.928416   0.974218              0.0  0.973268             0.0  \n",
       "6     0.942854   0.978359              0.0  0.978370             0.0  \n",
       "7     0.985037   0.994370              0.0  0.994347             0.0  \n",
       "8     0.987516   0.995330              0.0  0.995286             0.0  \n",
       "9     0.959804   0.985006              0.0  0.984868             0.0  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test models on color as well\n",
    "\n",
    "#define dfs for x and y\n",
    "y_data_col = wines_clean['color_cat']\n",
    "x_data_col = wines_clean.drop(['color_cat', 'quality_label', 'quality', 'color', 'index', 'is_red', 'is_white'], axis = 1) \n",
    "\n",
    "#split training data\n",
    "x_training_data_col, x_test_data_col, y_training_data_col, y_test_data_col = train_test_split(x_data_col, y_data_col, test_size = 0.2)\n",
    "\n",
    "deploy_models(x_training_data_col, x_test_data_col, y_training_data_col, y_test_data_col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Feature reduction (round 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: drop highly correlated features and drop features that do not help in the prediction.\n",
    "\n",
    "Resource: https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: -1.25858\n",
      "Feature: 1, Score: -7.67151\n",
      "Feature: 2, Score: 0.70605\n",
      "Feature: 3, Score: 0.13055\n",
      "Feature: 4, Score: -2.56912\n",
      "Feature: 5, Score: -0.04828\n",
      "Feature: 6, Score: 0.05840\n",
      "Feature: 7, Score: -0.23117\n",
      "Feature: 8, Score: -5.61695\n",
      "Feature: 9, Score: -6.43050\n",
      "Feature: 10, Score: 0.48703\n",
      "Feature: 11, Score: 0.41974\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGdCAYAAAAvwBgXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAW0klEQVR4nO3df2yVhf3o8U8LVr6OiRZoFOs0iqCjiohGnUQ3zZzinAqylKAuTEVQ43dOBZ2TZajp5n44L50TDSNmMoQ1C9kmmZvXbckwTraRqej8EfE34ZfABdFhe87958q9vQq06unnOe3rlSxZDyf9fGIP9N3nPH2emnK5XA4AgAS12QsAAH2XEAEA0ggRACCNEAEA0ggRACCNEAEA0ggRACCNEAEA0ggRACCNEAEA0vTPXqArNm7cGi5EDwDVoaYmYvDgT3fpuVURIuVyCBEA6IW8NQMApBEiAEAaIQIApBEiAEAaIQIApBEiAEAaIQIApBEiAEAaIQIApBEiAEAaIQIApBEiAECaqrjpHcVQW1sTtbU1KbNLpXKUSu58CNDbCBG6pLa2Jgbtt0/075dzEK29oxRbNm8XIwC9jBChS2pra6J/v9r47wdXxovrtvXo7OENA+Ou5jFRW1sjRAB6GSFCt7y4blusevN/Za8BQC/hZFUAII0QAQDSCBEAII0QAQDSCBEAII3fmgGgz8i6MKOLMu6aEAGgT8i8MKOLMu6aEAGgT8i6MKOLMu6eEAGgTynihRn78r28hAgAJOrr9/ISIgCQqK/fy0uIAEABFPEto57gOiIAQBohAgCkESIAQBohAgCkESIAQBohAgCkESIAQBohAgCkcUEzql5fvkcDQLUTIlS1vn6PBoBq1yMhsnDhwpg/f36sX78+jjjiiPjWt74Vxx9/fE+Mppfr6/do6G0c3YK+p+IhsmzZsmhpaYnvfOc7cdxxx8WDDz4Yl19+eTz00EMxbNiwSo+nj+ir92joTRzdgr6p4iGyYMGCmDhxYkyaNCkiIm6++eb461//GosWLYrrrruu0uOBKuHoFvRNFQ2RHTt2xKpVq2LatGmdHj/llFNi5cqVlRzdJVmHgfd0CLjIh6eHNwzswW26PrOoe3mNdVatb38U9e9kUfeKKPZrrKf/vejqvKL+O1ZpFQ2RTZs2RUdHRwwePLjT40OGDIn169dXcvQeZR4G3t0h4KIeni6VytHeUYq7msek7fVh/72KuleE11h39yrq17KofyeLuleE19iu9trVvxVFfe33lB45WbWmpnMVl8vlDzzW07IOA+/pEHBRD0+XSuXYsnl74X76KupeEcV/jf2P//lCvLH5nR7b66D9/iuuOeOIXe5V1K9lUf9OFnWvzN32tFfma2x3/1YU9bXfUyoaIvvvv3/069cvNmzY0OnxjRs3xpAhQyo5usuKepJjEffKfrHuSlH3el/Rvpbv//R1zRlH9PjsPf3kVeSvZdG+ju8r6l4RxdytqK+xou7VEyoaInV1dTFq1KhYvnx5fPGLX9z5+GOPPRZnnHFGJUcDu1DUnwqBvqnib81MnTo1Zs6cGU1NTTFmzJhYvHhxrFmzJpqbmys9GtgFQQAURcVDZPz48bFp06a4++67Y926dTFixIi4995746CDDqr0aACg4HrkZNUpU6bElClTemIUAFBF3H0XAEjjpncAXdBXLzYFlSZEAHajr19sCipNiADsRl+/2BRUmhAB2AMxAJXjZFUAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAIE3/7AWyDW8Y2KvnAUCR9dkQKZXK0d5Riruax/T47PaOUpRK5R6fCwBF06dDZMvm7VFbW5MyW4gAQB8OkQhBAADZnKwKAKQRIgBAGiECAKQRIgBAGiECAKQRIgBAGiECAKQRIgBAGiECAKQRIgBAGiECAKQRIgBAGiECAKQRIgBAGiECAKQRIgBAGiECAKQRIgBAGiECAKQRIgBAGiECAKQRIgBAGiECAKTpX6lP/Prrr8fdd98djz/+eGzYsCEaGhriK1/5SkyfPj3q6uoqNRYAqCIVC5GXXnopyuVyzJkzJw455JB4/vnn45Zbbol33nknZs2aVamxAEAVqViInHrqqXHqqafu/Pjggw+O1atXx6JFi4QIABARPXyOyNatW2PQoEE9ORIAKLAeC5FXX301HnjggZg8eXJPjQQACq7bb83MnTs3Wltbd/uctra2OProo3d+vHbt2rjsssvirLPOikmTJnV/SwCgV+p2iEyZMiXGjx+/2+c0Njbu/P9r166NSy65JI499ti49dZbu78hANBrdTtE6uvro76+vkvPfT9CRo0aFS0tLVFb67IlAMD/VbHfmlm7dm1cfPHFceCBB8asWbPirbfe2vlnQ4cOrdRYAKCKVCxEli9fHq+88kq88sornX6NNyLiueeeq9RYAKCKVCxEJkyYEBMmTKjUpwcAegEnbQAAaYQIAJBGiAAAaYQIAJBGiAAAaYQIAJBGiAAAaYQIAJBGiAAAaYQIAJBGiAAAaYQIAJBGiAAAaYQIAJBGiAAAafpnLwC93fCGgb16HsDHIUSgQkqlcrR3lOKu5jE9Pru9oxSlUrnH5wJ0lxCBCimVyrFl8/aora1JmS1EgGogRKCCBAHA7jlZFQBII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDRCBABII0QAgDQ9EiI7duyI8847L0aOHBnPPvtsT4wEAKpAj4TIHXfcEQ0NDT0xCgCoIhUPkb/85S+xfPnymDVrVqVHAQBVpn8lP/mGDRvilltuiZ/+9KcxYMCASo4CAKpQxY6IlMvluPHGG6O5uTmOPvroSo0BAKpYt4+IzJ07N1pbW3f7nLa2tli5cmVs27Ytrrjiio+8HADQu3U7RKZMmRLjx4/f7XMaGxvjZz/7WfzrX//6wNGQiRMnxrnnnhvf//73uzsaAOhluh0i9fX1UV9fv8fnffvb345vfOMbOz9et25dXHrppXHnnXfG6NGjuzsWAOiFKnay6rBhwzp9vM8++0RExGc+85k44IADKjUWAKgirqwKAKSp6K/v/r8aGxvjueee66lxAEAVcEQEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEjTP3sBAD664Q0D+8RMei8hAlCFSqVytHeU4q7mMSnz2ztKUSqVU2bTuwgRgCpUKpVjy+btUVtbkzZfiPBJECIAVUoM0Bs4WRUASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0FQ+RP//5zzFp0qQ45phj4sQTT4yrr7660iMBgCpR0SurPvzww3HLLbfEtddeGyeddFKUy+V4/vnnKzkSAKgiFQuR9vb2uP322+OGG26ISZMm7Xz8sMMOq9RIAKDKVOytmWeeeSbWrl0btbW1cf7558e4cePisssuixdeeKFSIwGAKlOxEHnttdciIqK1tTVmzJgR99xzTwwaNCguuuii2Lx5c6XGAgBVpNtvzcydOzdaW1t3+5y2trYolUoRETF9+vT40pe+FBERLS0tceqpp8bvf//7aG5u/gjrAgC9SbdDZMqUKTF+/PjdPqexsTHefvvtiIg4/PDDdz5eV1cXBx98cKxZs6a7YwGAXqjbIVJfXx/19fV7fF5TU1PU1dXF6tWr4/jjj4+IiPfeey/eeOONGDZsWPc3BQB6nYr91szAgQOjubk55s6dGwceeGAMGzYs5s+fHxERZ511VqXGAgBVpKLXEZk5c2b0798/Zs6cGe+++26MHj067r///hg0aFAlxwIAVaKiIbLXXnvFrFmzYtasWZUcAwBUKfeaAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAII0QAQDSCBEAIE1FL/EOQN81vGFgr57HJ0OIAPCJKpXK0d5Riruax/T47PaOUpRK5R6fy0cnRAD4RJVK5diyeXvU1takzBYi1UWIAPCJEwR0lZNVAYA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0QgQASCNEAIA0/bMX4MMNbxjYJ2YC0LfVlMvlcvYSe7Jhw9Yo/pafjNramhi03z7Rv1/Owar2jlJs2bw9SqU+8h8cgE9cTU3EkCGf7tJzHREpmFKpHFs2b4/a2pq0+SIEgJ4iRApIDADQVzhZFQBII0QAgDRCBABII0QAgDRCBABII0QAgDQVDZHVq1fHjBkz4sQTT4zjjjsumpub4/HHH6/kSACgilQ0RK644oro6OiI+++/P37961/HUUcdFdOnT4/169dXciwAUCUqFiJvvfVWvPLKKzFt2rQ48sgj49BDD43rrrsu3nnnnXjxxRcrNRYAqCIVC5H9998/Dj/88Fi6dGls37492tvbY/HixTFkyJAYNWpUpcYCAFWkoje9W7t2bcyYMSOeeeaZqK2tjcGDB8e9994bRx11VLc+T1+66R0AVLvu3PSu2yEyd+7caG1t3e1z2traoqmpKa688spob2+P6dOnx4ABA+JXv/pVPProo9HW1hYNDQ1dnilEAKB6VDRE3nrrrdi0adNun9PY2Bj//Oc/4+tf/3qsWLEiBg4cuPPPzjzzzLjwwgtj2rRpXZ4pRACgenQnRLp99936+vqor6/f4/Peeeed/7NM59vZ19TURKlU6tbM/+9TAAAF1p3v290Oka469thjY999940bb7wxrrrqqth7771jyZIl8cYbb8TnP//5bn2uwYO7VlUAQHWp6MmqTz31VPzkJz+Jp59+Ot5777044ogj4sorr4zTTjutUiMBgCpS0RABANgd95oBANIIEQAgjRABANIIEQAgjRABANIIEQAgjRABANIIEQAgjRDppoULF8bpp58eRx99dEyYMCH+/ve/Z69UWPPmzYuJEyfGmDFj4uSTT44rr7wyXnrppey1qsa8efNi5MiRcfvtt2evUmhr166N66+/Pk488cQYPXp0nHfeefH0009nr1VI7e3tceedd8bpp58exxxzTJxxxhnR2tra7ft/9WYrVqyI6dOnx7hx42LkyJHxyCOPdPrzcrkcc+fOjXHjxsUxxxwTF198cbzwwgtJ2/YOQqQbli1bFi0tLTFjxoxYunRpjB07Ni6//PJ48803s1crpCeeeCKmTJkSS5YsiQULFkRHR0dceumlsX379uzVCu/JJ5+MxYsXx8iRI7NXKbQtW7bE5MmTY6+99or77rsvHnroobjxxhtj3333zV6tkO6777548MEHY/bs2bFs2bK44YYbYv78+fGLX/wie7XC2L59e4wcOTJmz579oX9+3333xYIFC2L27NnR1tYWQ4YMialTp8a2bdt6eNNepEyXXXjhheXZs2d3euyss84q//CHP0zaqLps3LixPGLEiPITTzyRvUqhbdu2rXzmmWeWly9fXr7ooovKt912W/ZKhfWDH/ygPHny5Ow1qsa0adPKN910U6fHrr766vL111+ftFGxjRgxovzHP/5x58elUql8yimnlOfNm7fzsf/85z/lsWPHlhctWpSxYq/giEgX7dixI1atWhXjxo3r9Pgpp5wSK1euTNqqumzdujUiIgYNGpS8SbHNmTMnTjvttPjc5z6XvUrhPfroo9HU1BTXXHNNnHzyyXH++efHkiVLstcqrLFjx8bjjz8eq1evjoiIf//73/GPf/zDjUi76PXXX4/169d3+j5QV1cXJ5xwgu8DH0P/7AWqxaZNm6KjoyMGDx7c6fEhQ4bE+vXrk7aqHuVyOVpaWmLs2LExYsSI7HUK66GHHopnnnkm2traslepCq+99losWrQopk6dGtOnT48nn3wybrvttqirq4vzzz8/e73Cufzyy2Pr1q1x9tlnR79+/aKjoyOuvfba+PKXv5y9WlV4/9/6D/s+4C36j06IdFNNTU2nj8vl8gce44PmzJkTzz//fPzyl7/MXqWw1qxZE7fffnv8/Oc/j7333jt7napQLpejqakpvvnNb0ZExGc/+9l48cUXY9GiRULkQyxbtix+85vfxI9+9KMYPnx4PPvss9HS0hINDQ1xwQUXZK9XNT7s+wAfnRDpov333z/69esXGzZs6PT4xo0bY8iQIUlbVYdbb701Hn300XjggQfigAMOyF6nsFatWhUbN26MCRMm7Hyso6MjVqxYEQsXLoynnnoq+vXrl7hh8QwdOjQOP/zwTo8ddthh8fDDDydtVGx33HFHTJs2Lc4555yIiBg5cmS8+eabMW/ePCHSBUOHDo2IiA0bNkRDQ8POx30f+HicI9JFdXV1MWrUqFi+fHmnxx977LEYM2ZM0lbFVi6XY86cOfGHP/wh7r///jj44IOzVyq0k046KX7729/G0qVLd/6vqakpzj333Fi6dKkI+RDHHXfczvMd3vfyyy/HQQcdlLRRsb377rsf+Gm+X79+fqLvosbGxhg6dGin7wM7duyIFStW+D7wMTgi0g1Tp06NmTNnRlNTU4wZMyYWL14ca9asiebm5uzVCum73/1u/O53v4u77747PvWpT+18f/XTn/50DBgwIHm74hk4cOAHzp/ZZ599Yr/99nNezS587Wtfi8mTJ8c999wTZ599djz55JOxZMmSmDNnTvZqhfSFL3wh7rnnnhg2bNjOt2YWLFgQEydOzF6tMN5+++149dVXd378+uuvx7PPPhuDBg2KYcOGxSWXXBLz5s2LQw89NA455JCYN29eDBgwwHk2H0NNWQp3y8KFC2P+/Pmxbt26GDFiRNx0001xwgknZK9VSLu6BkZLS0untx/YtYsvvjiOPPLIuPnmm7NXKaw//elP8eMf/zhefvnlaGxsjKlTp8ZXv/rV7LUKadu2bXHXXXfFI488Ehs3boyGhoY455xz4qqrroq6urrs9Qrhb3/7W1xyySUfePyCCy6I733ve1Eul6O1tTUWL14cW7ZsidGjR8fs2bP9sPAxCBEAII1zRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEgjRACANEIEAEjzvwF/zP+LYHOExQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get importance\n",
    "importance = lr.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    " print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests with removed (dropped) features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Remove free_sulfur_dioxide, density, pH and residual_sugar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove free_sulfur_dioxide, density (strongly correlated to other vars). remove pH and residual_sugar (weakly corellated with target)\n",
    "\n",
    "#define dfs for x and y\n",
    "y_data = wines_clean['quality_label_cat']\n",
    "x_data1 = wines_clean.drop(['quality_label_cat', 'pH', 'free_sulfur_dioxide', 'density', 'residual_sugar', 'color_cat', 'quality_label', 'quality', 'color', 'index'], axis = 1) \n",
    "\n",
    "#split training data\n",
    "x_training_data1, x_test_data1, y_training_data1, y_test_data1 = train_test_split(x_data1, y_data, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_40457/3230836472.py:3: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  display('Dropped all:', dropped_all, dropped_all.mean())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Dropped all:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.727444</td>\n",
       "      <td>0.422363</td>\n",
       "      <td>0.708125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714945</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.733083</td>\n",
       "      <td>0.430733</td>\n",
       "      <td>0.714254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.719404</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.731203</td>\n",
       "      <td>0.423597</td>\n",
       "      <td>0.712884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.716344</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.253681</td>\n",
       "      <td>0.630566</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.627579</td>\n",
       "      <td>0.037736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.640977</td>\n",
       "      <td>0.241056</td>\n",
       "      <td>0.621404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.628676</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.108792</td>\n",
       "      <td>0.643313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409431</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.662594</td>\n",
       "      <td>0.293328</td>\n",
       "      <td>0.656370</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.654796</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.431556</td>\n",
       "      <td>0.712978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.719426</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.747180</td>\n",
       "      <td>0.473682</td>\n",
       "      <td>0.740316</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.738760</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.416701</td>\n",
       "      <td>0.702825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.711177</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.727444   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.733083   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.731203   \n",
       "3                           DecisionTreeClassifier()        0.625000   \n",
       "4                             KNeighborsClassifier()        0.640977   \n",
       "5                           LinearSVC(max_iter=3000)        0.473684   \n",
       "6                                       GaussianNB()        0.662594   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.732143   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.747180   \n",
       "9                                    MLPClassifier()        0.721805   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  \n",
       "0     0.422363   0.708125         0.000000  0.714945        0.000000  \n",
       "1     0.430733   0.714254         0.000000  0.719404        0.000000  \n",
       "2     0.423597   0.712884         0.000000  0.716344        0.000000  \n",
       "3     0.253681   0.630566         0.034483  0.627579        0.037736  \n",
       "4     0.241056   0.621404         0.000000  0.628676        0.000000  \n",
       "5     0.108792   0.643313         0.000000  0.409431        0.000000  \n",
       "6     0.293328   0.656370         0.090909  0.654796        0.086957  \n",
       "7     0.431556   0.712978         0.000000  0.719426        0.000000  \n",
       "8     0.473682   0.740316         0.500000  0.738760        0.076923  \n",
       "9     0.416701   0.702825         0.000000  0.711177        0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Accuracy Score     0.679511\n",
       "Cohen Kappa        0.349549\n",
       "Precision          0.684304\n",
       "Precision: high    0.062539\n",
       "F1 Score           0.664054\n",
       "F1 Score: high     0.020162\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40457/3230836472.py:5: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  display('None dropped:', orig_results, orig_results.mean())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'None dropped:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.716165</td>\n",
       "      <td>0.398906</td>\n",
       "      <td>0.692765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.703271</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.429021</td>\n",
       "      <td>0.708343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.718194</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.718985</td>\n",
       "      <td>0.397935</td>\n",
       "      <td>0.694813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.615602</td>\n",
       "      <td>0.237768</td>\n",
       "      <td>0.624141</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.619636</td>\n",
       "      <td>0.109589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.636278</td>\n",
       "      <td>0.227523</td>\n",
       "      <td>0.612339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622866</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>0.545113</td>\n",
       "      <td>0.206846</td>\n",
       "      <td>0.665134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519474</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.601504</td>\n",
       "      <td>0.217438</td>\n",
       "      <td>0.615932</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.608152</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.412615</td>\n",
       "      <td>0.698718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709353</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.738722</td>\n",
       "      <td>0.449609</td>\n",
       "      <td>0.730086</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.727771</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.730263</td>\n",
       "      <td>0.407678</td>\n",
       "      <td>0.708516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.716165   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.732143   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.718985   \n",
       "3                           DecisionTreeClassifier()        0.615602   \n",
       "4                             KNeighborsClassifier()        0.636278   \n",
       "5                           LinearSVC(max_iter=3000)        0.545113   \n",
       "6                                       GaussianNB()        0.601504   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.721805   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.738722   \n",
       "9                                    MLPClassifier()        0.730263   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  \n",
       "0     0.398906   0.692765         0.000000  0.703271        0.000000  \n",
       "1     0.429021   0.708343         0.000000  0.718194        0.000000  \n",
       "2     0.397935   0.694813         0.000000  0.704100        0.000000  \n",
       "3     0.237768   0.624141         0.093023  0.619636        0.109589  \n",
       "4     0.227523   0.612339         0.000000  0.622866        0.000000  \n",
       "5     0.206846   0.665134         0.000000  0.519474        0.000000  \n",
       "6     0.217438   0.615932         0.103448  0.608152        0.136364  \n",
       "7     0.412615   0.698718         0.000000  0.709353        0.000000  \n",
       "8     0.449609   0.730086         0.500000  0.727771        0.062500  \n",
       "9     0.407678   0.708516         0.000000  0.710398        0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Accuracy Score     0.675658\n",
       "Cohen Kappa        0.338534\n",
       "Precision          0.675079\n",
       "Precision: high    0.069647\n",
       "F1 Score           0.664321\n",
       "F1 Score: high     0.030845\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#call function to test refined dataset\n",
    "dropped_all = deploy_models(x_training_data1, x_test_data1, y_training_data1, y_test_data1)\n",
    "display('Dropped all:', dropped_all, dropped_all.mean())\n",
    "#compare to unmodified\n",
    "display('None dropped:', orig_results, orig_results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the models actually performed worse on average"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Remove only pH and residual_sugar (weakly correlated with target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_40457/944908654.py:12: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  display(drop_ph_sugar.mean(), 'None dropped:', orig_results.mean())\n",
      "/tmp/ipykernel_40457/944908654.py:12: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  display(drop_ph_sugar.mean(), 'None dropped:', orig_results.mean())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Accuracy Score     0.685056\n",
       "Cohen Kappa        0.346417\n",
       "Precision          0.675573\n",
       "Precision: high    0.068453\n",
       "F1 Score           0.674455\n",
       "F1 Score: high     0.028842\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'None dropped:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Accuracy Score     0.675658\n",
       "Cohen Kappa        0.338534\n",
       "Precision          0.675079\n",
       "Precision: high    0.069647\n",
       "F1 Score           0.664321\n",
       "F1 Score: high     0.030845\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define dfs for x and y\n",
    "y_data = wines_clean['quality_label_cat']\n",
    "x_data2 = wines_clean.drop(['quality_label_cat', 'pH','residual_sugar', 'color_cat', 'quality_label', 'quality', 'color', 'index'], axis = 1) \n",
    "\n",
    "#split training data\n",
    "x_training_data2, x_test_data2, y_training_data2, y_test_data2 = train_test_split(x_data, y_data, test_size = 0.2, random_state=4)\n",
    "\n",
    "#call function to test refined dataset\n",
    "drop_ph_sugar = deploy_models(x_training_data2, x_test_data2, y_training_data2, y_test_data2)\n",
    "\n",
    "#compare test results (average of all models)\n",
    "display(drop_ph_sugar.mean(), 'None dropped:', orig_results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no large changes, but prediction of 'high' target seems better"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Remove only free_sulfur_dioxide and density (strongly correlated to other vars, possibly redundant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_40457/2618298598.py:14: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  display(drop_so2_density, drop_so2_density.mean(), orig_results, orig_results.mean())\n",
      "/tmp/ipykernel_40457/2618298598.py:14: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  display(drop_so2_density, drop_so2_density.mean(), orig_results, orig_results.mean())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.730263</td>\n",
       "      <td>0.426173</td>\n",
       "      <td>0.706535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.716636</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.405483</td>\n",
       "      <td>0.697772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707392</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.728383</td>\n",
       "      <td>0.420715</td>\n",
       "      <td>0.704417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.636278</td>\n",
       "      <td>0.287375</td>\n",
       "      <td>0.648202</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.640833</td>\n",
       "      <td>0.144928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.661654</td>\n",
       "      <td>0.286796</td>\n",
       "      <td>0.639745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.649925</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>0.419173</td>\n",
       "      <td>0.061980</td>\n",
       "      <td>0.658286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320726</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.638158</td>\n",
       "      <td>0.240365</td>\n",
       "      <td>0.622919</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.628979</td>\n",
       "      <td>0.048780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.728383</td>\n",
       "      <td>0.421591</td>\n",
       "      <td>0.704590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714592</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.745301</td>\n",
       "      <td>0.464803</td>\n",
       "      <td>0.736855</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.734649</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.677632</td>\n",
       "      <td>0.244168</td>\n",
       "      <td>0.668133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.629941</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.730263   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.721805   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.728383   \n",
       "3                           DecisionTreeClassifier()        0.636278   \n",
       "4                             KNeighborsClassifier()        0.661654   \n",
       "5                           LinearSVC(max_iter=3000)        0.419173   \n",
       "6                                       GaussianNB()        0.638158   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.728383   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.745301   \n",
       "9                                    MLPClassifier()        0.677632   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  \n",
       "0     0.426173   0.706535         0.000000  0.716636        0.000000  \n",
       "1     0.405483   0.697772         0.000000  0.707392        0.000000  \n",
       "2     0.420715   0.704417         0.000000  0.714290        0.000000  \n",
       "3     0.287375   0.648202         0.128205  0.640833        0.144928  \n",
       "4     0.286796   0.639745         0.000000  0.649925        0.000000  \n",
       "5     0.061980   0.658286         0.000000  0.320726        0.000000  \n",
       "6     0.240365   0.622919         0.090909  0.628979        0.048780  \n",
       "7     0.421591   0.704590         0.000000  0.714592        0.000000  \n",
       "8     0.464803   0.736855         0.500000  0.734649        0.062500  \n",
       "9     0.244168   0.668133         0.000000  0.629941        0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Accuracy Score     0.668703\n",
       "Cohen Kappa        0.325945\n",
       "Precision          0.678745\n",
       "Precision: high    0.071911\n",
       "F1 Score           0.645796\n",
       "F1 Score: high     0.025621\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.716165</td>\n",
       "      <td>0.398906</td>\n",
       "      <td>0.692765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.703271</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.429021</td>\n",
       "      <td>0.708343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.718194</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.718985</td>\n",
       "      <td>0.397935</td>\n",
       "      <td>0.694813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.615602</td>\n",
       "      <td>0.237768</td>\n",
       "      <td>0.624141</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.619636</td>\n",
       "      <td>0.109589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.636278</td>\n",
       "      <td>0.227523</td>\n",
       "      <td>0.612339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622866</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>0.545113</td>\n",
       "      <td>0.206846</td>\n",
       "      <td>0.665134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519474</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.601504</td>\n",
       "      <td>0.217438</td>\n",
       "      <td>0.615932</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.608152</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.412615</td>\n",
       "      <td>0.698718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709353</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.738722</td>\n",
       "      <td>0.449609</td>\n",
       "      <td>0.730086</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.727771</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.730263</td>\n",
       "      <td>0.407678</td>\n",
       "      <td>0.708516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.716165   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.732143   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.718985   \n",
       "3                           DecisionTreeClassifier()        0.615602   \n",
       "4                             KNeighborsClassifier()        0.636278   \n",
       "5                           LinearSVC(max_iter=3000)        0.545113   \n",
       "6                                       GaussianNB()        0.601504   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.721805   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.738722   \n",
       "9                                    MLPClassifier()        0.730263   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  \n",
       "0     0.398906   0.692765         0.000000  0.703271        0.000000  \n",
       "1     0.429021   0.708343         0.000000  0.718194        0.000000  \n",
       "2     0.397935   0.694813         0.000000  0.704100        0.000000  \n",
       "3     0.237768   0.624141         0.093023  0.619636        0.109589  \n",
       "4     0.227523   0.612339         0.000000  0.622866        0.000000  \n",
       "5     0.206846   0.665134         0.000000  0.519474        0.000000  \n",
       "6     0.217438   0.615932         0.103448  0.608152        0.136364  \n",
       "7     0.412615   0.698718         0.000000  0.709353        0.000000  \n",
       "8     0.449609   0.730086         0.500000  0.727771        0.062500  \n",
       "9     0.407678   0.708516         0.000000  0.710398        0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Accuracy Score     0.675658\n",
       "Cohen Kappa        0.338534\n",
       "Precision          0.675079\n",
       "Precision: high    0.069647\n",
       "F1 Score           0.664321\n",
       "F1 Score: high     0.030845\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define dfs for x and y\n",
    "y_data = wines_clean['quality_label_cat']\n",
    "x_data3 = wines_clean.drop(['quality_label_cat', 'free_sulfur_dioxide', 'density',  'color_cat', 'quality_label', 'quality', 'color', 'index'], axis = 1) \n",
    "\n",
    "#split training data\n",
    "x_training_data3, x_test_data3, y_training_data3, y_test_data3 = train_test_split(x_data3, y_data, test_size = 0.2, random_state=4)\n",
    "\n",
    "deploy_models(x_training_data3, x_test_data3, y_training_data3, y_test_data3)\n",
    "\n",
    "#call function to test refined dataset\n",
    "drop_so2_density = deploy_models(x_training_data3, x_test_data3, y_training_data3, y_test_data3)\n",
    "\n",
    "#compare test results (average of all models)\n",
    "display(drop_so2_density, drop_so2_density.mean(), orig_results, orig_results.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "slight improvement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature scaling \n",
    "Note: should be done after splitting to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit scaler on training data eg MinMax and save to a descriptively named variable eg norm/stand..\n",
    "# norm = MinMaxScaler().fit(X_train)\n",
    "\n",
    "## transform training data\n",
    "# X_train_norm = norm.transform(X_train)\n",
    "\n",
    "## transform testing data\n",
    "# X_test_norm = norm.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'After normalisation with MinMaxScaler: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.727444</td>\n",
       "      <td>0.422211</td>\n",
       "      <td>0.703942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714369</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.726504</td>\n",
       "      <td>0.420510</td>\n",
       "      <td>0.703045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.713508</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.727444</td>\n",
       "      <td>0.422211</td>\n",
       "      <td>0.703942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714369</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.600564</td>\n",
       "      <td>0.212986</td>\n",
       "      <td>0.612207</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.605971</td>\n",
       "      <td>0.053333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.397506</td>\n",
       "      <td>0.692863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.701420</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>0.727444</td>\n",
       "      <td>0.422791</td>\n",
       "      <td>0.704027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714522</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.587406</td>\n",
       "      <td>0.205315</td>\n",
       "      <td>0.610401</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.597562</td>\n",
       "      <td>0.138614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.412615</td>\n",
       "      <td>0.698718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709353</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.748120</td>\n",
       "      <td>0.470464</td>\n",
       "      <td>0.739602</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.737391</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.747180</td>\n",
       "      <td>0.474080</td>\n",
       "      <td>0.725824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.736341</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.727444   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.726504   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.727444   \n",
       "3                           DecisionTreeClassifier()        0.600564   \n",
       "4                             KNeighborsClassifier()        0.710526   \n",
       "5                           LinearSVC(max_iter=3000)        0.727444   \n",
       "6                                       GaussianNB()        0.587406   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.721805   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.748120   \n",
       "9                                    MLPClassifier()        0.747180   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  \n",
       "0     0.422211   0.703942         0.000000  0.714369        0.000000  \n",
       "1     0.420510   0.703045         0.000000  0.713508        0.000000  \n",
       "2     0.422211   0.703942         0.000000  0.714369        0.000000  \n",
       "3     0.212986   0.612207         0.044444  0.605971        0.053333  \n",
       "4     0.397506   0.692863         0.000000  0.701420        0.000000  \n",
       "5     0.422791   0.704027         0.000000  0.714522        0.000000  \n",
       "6     0.205315   0.610401         0.098592  0.597562        0.138614  \n",
       "7     0.412615   0.698718         0.000000  0.709353        0.000000  \n",
       "8     0.470464   0.739602         0.500000  0.737391        0.062500  \n",
       "9     0.474080   0.725824         0.000000  0.736341        0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Unmodified: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.716165</td>\n",
       "      <td>0.398906</td>\n",
       "      <td>0.692765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.703271</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.429021</td>\n",
       "      <td>0.708343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.718194</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.718985</td>\n",
       "      <td>0.397935</td>\n",
       "      <td>0.694813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.615602</td>\n",
       "      <td>0.237768</td>\n",
       "      <td>0.624141</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.619636</td>\n",
       "      <td>0.109589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.636278</td>\n",
       "      <td>0.227523</td>\n",
       "      <td>0.612339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622866</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>0.545113</td>\n",
       "      <td>0.206846</td>\n",
       "      <td>0.665134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519474</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.601504</td>\n",
       "      <td>0.217438</td>\n",
       "      <td>0.615932</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.608152</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.412615</td>\n",
       "      <td>0.698718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709353</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.738722</td>\n",
       "      <td>0.449609</td>\n",
       "      <td>0.730086</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.727771</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.730263</td>\n",
       "      <td>0.407678</td>\n",
       "      <td>0.708516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.716165   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.732143   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.718985   \n",
       "3                           DecisionTreeClassifier()        0.615602   \n",
       "4                             KNeighborsClassifier()        0.636278   \n",
       "5                           LinearSVC(max_iter=3000)        0.545113   \n",
       "6                                       GaussianNB()        0.601504   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.721805   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.738722   \n",
       "9                                    MLPClassifier()        0.730263   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  \n",
       "0     0.398906   0.692765         0.000000  0.703271        0.000000  \n",
       "1     0.429021   0.708343         0.000000  0.718194        0.000000  \n",
       "2     0.397935   0.694813         0.000000  0.704100        0.000000  \n",
       "3     0.237768   0.624141         0.093023  0.619636        0.109589  \n",
       "4     0.227523   0.612339         0.000000  0.622866        0.000000  \n",
       "5     0.206846   0.665134         0.000000  0.519474        0.000000  \n",
       "6     0.217438   0.615932         0.103448  0.608152        0.136364  \n",
       "7     0.412615   0.698718         0.000000  0.709353        0.000000  \n",
       "8     0.449609   0.730086         0.500000  0.727771        0.062500  \n",
       "9     0.407678   0.708516         0.000000  0.710398        0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit scaler on training data\n",
    "norm = MinMaxScaler().fit(x_training_data)\n",
    "\n",
    "# transform training data\n",
    "x_training_norm = norm.transform(x_training_data)\n",
    "\n",
    "# transform testing data\n",
    "x_test_norm = norm.transform(x_test_data)\n",
    "\n",
    "# show normalised df\n",
    "#display(pd.DataFrame(x_test_norm))\n",
    "\n",
    "# run these through the testing function\n",
    "norm_results = deploy_models(x_training_norm, x_test_norm, y_training_data, y_test_data)\n",
    "display('After normalisation with MinMaxScaler: ', norm_results)\n",
    "# compare to original\n",
    "display('Unmodified: ', orig_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardisation with RobustScaler (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'After standardisation with RobustScaler: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.728383</td>\n",
       "      <td>0.425645</td>\n",
       "      <td>0.705102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715684</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.726504</td>\n",
       "      <td>0.421090</td>\n",
       "      <td>0.703134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.713660</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.728383</td>\n",
       "      <td>0.425645</td>\n",
       "      <td>0.705102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.715684</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.612782</td>\n",
       "      <td>0.233957</td>\n",
       "      <td>0.622156</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.617127</td>\n",
       "      <td>0.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.700188</td>\n",
       "      <td>0.373421</td>\n",
       "      <td>0.681427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690365</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>0.727444</td>\n",
       "      <td>0.422791</td>\n",
       "      <td>0.704027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714522</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.587406</td>\n",
       "      <td>0.205315</td>\n",
       "      <td>0.610401</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.597562</td>\n",
       "      <td>0.138614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.412615</td>\n",
       "      <td>0.698718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709353</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.746241</td>\n",
       "      <td>0.465448</td>\n",
       "      <td>0.737565</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.735262</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.742481</td>\n",
       "      <td>0.468575</td>\n",
       "      <td>0.723415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.732741</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.728383   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.726504   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.728383   \n",
       "3                           DecisionTreeClassifier()        0.612782   \n",
       "4                             KNeighborsClassifier()        0.700188   \n",
       "5                           LinearSVC(max_iter=3000)        0.727444   \n",
       "6                                       GaussianNB()        0.587406   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.721805   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.746241   \n",
       "9                                    MLPClassifier()        0.742481   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  \n",
       "0     0.425645   0.705102         0.000000  0.715684        0.000000  \n",
       "1     0.421090   0.703134         0.000000  0.713660        0.000000  \n",
       "2     0.425645   0.705102         0.000000  0.715684        0.000000  \n",
       "3     0.233957   0.622156         0.130435  0.617127        0.157895  \n",
       "4     0.373421   0.681427         0.000000  0.690365        0.000000  \n",
       "5     0.422791   0.704027         0.000000  0.714522        0.000000  \n",
       "6     0.205315   0.610401         0.098592  0.597562        0.138614  \n",
       "7     0.412615   0.698718         0.000000  0.709353        0.000000  \n",
       "8     0.465448   0.737565         0.500000  0.735262        0.062500  \n",
       "9     0.468575   0.723415         0.000000  0.732741        0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Unmodified: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.716165</td>\n",
       "      <td>0.398906</td>\n",
       "      <td>0.692765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.703271</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.429021</td>\n",
       "      <td>0.708343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.718194</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.718985</td>\n",
       "      <td>0.397935</td>\n",
       "      <td>0.694813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.615602</td>\n",
       "      <td>0.237768</td>\n",
       "      <td>0.624141</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.619636</td>\n",
       "      <td>0.109589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.636278</td>\n",
       "      <td>0.227523</td>\n",
       "      <td>0.612339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622866</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>0.545113</td>\n",
       "      <td>0.206846</td>\n",
       "      <td>0.665134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519474</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.601504</td>\n",
       "      <td>0.217438</td>\n",
       "      <td>0.615932</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.608152</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.412615</td>\n",
       "      <td>0.698718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709353</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.738722</td>\n",
       "      <td>0.449609</td>\n",
       "      <td>0.730086</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.727771</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.730263</td>\n",
       "      <td>0.407678</td>\n",
       "      <td>0.708516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710398</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.716165   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.732143   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.718985   \n",
       "3                           DecisionTreeClassifier()        0.615602   \n",
       "4                             KNeighborsClassifier()        0.636278   \n",
       "5                           LinearSVC(max_iter=3000)        0.545113   \n",
       "6                                       GaussianNB()        0.601504   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.721805   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.738722   \n",
       "9                                    MLPClassifier()        0.730263   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  \n",
       "0     0.398906   0.692765         0.000000  0.703271        0.000000  \n",
       "1     0.429021   0.708343         0.000000  0.718194        0.000000  \n",
       "2     0.397935   0.694813         0.000000  0.704100        0.000000  \n",
       "3     0.237768   0.624141         0.093023  0.619636        0.109589  \n",
       "4     0.227523   0.612339         0.000000  0.622866        0.000000  \n",
       "5     0.206846   0.665134         0.000000  0.519474        0.000000  \n",
       "6     0.217438   0.615932         0.103448  0.608152        0.136364  \n",
       "7     0.412615   0.698718         0.000000  0.709353        0.000000  \n",
       "8     0.449609   0.730086         0.500000  0.727771        0.062500  \n",
       "9     0.407678   0.708516         0.000000  0.710398        0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# StandardScaler: doesn't work correctly in the presence of outliers.\n",
    "# RobustScaler: uses 1st and 3rd quantiles so is better for outliers.\n",
    "\n",
    "# fit scaler on training data\n",
    "robust = RobustScaler().fit(x_training_data)\n",
    "\n",
    "# transform training data\n",
    "x_training_robust = robust.transform(x_training_data)\n",
    "\n",
    "# transform testing data\n",
    "x_test_robust = robust.transform(x_test_data)\n",
    "\n",
    "# run these through the testing function\n",
    "robust_results = deploy_models(x_training_robust, x_test_robust, y_training_data, y_test_data)\n",
    "display('After standardisation with RobustScaler: ', robust_results)\n",
    "# compare to original\n",
    "display('Unmodified: ', orig_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameter tuning and model improvement "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run k-Fold cross-validation on all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>is_red</th>\n",
       "      <th>is_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>67.0</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.8</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>168.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>5.6</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.028</td>\n",
       "      <td>110.0</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.028</td>\n",
       "      <td>98.0</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5320 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0               7.8              0.88         0.00             2.6      0.098   \n",
       "1               7.8              0.76         0.04             2.3      0.092   \n",
       "2               9.8              0.28         0.56             1.9      0.075   \n",
       "3               7.4              0.70         0.00             1.9      0.076   \n",
       "4               7.4              0.66         0.00             1.8      0.075   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "5315            6.2              0.21         0.29             1.6      0.039   \n",
       "5316            6.6              0.32         0.36             8.0      0.047   \n",
       "5317            6.5              0.24         0.19             1.2      0.041   \n",
       "5318            5.6              0.29         0.30             1.1      0.028   \n",
       "5319            6.0              0.21         0.38             0.9      0.028   \n",
       "\n",
       "      total_sulfur_dioxide    pH  sulfates  alcohol  is_red  is_white  \n",
       "0                     67.0  3.20      0.68      9.8     1.0       0.0  \n",
       "1                     54.0  3.26      0.65      9.8     1.0       0.0  \n",
       "2                     60.0  3.16      0.58      9.8     1.0       0.0  \n",
       "3                     34.0  3.51      0.56      9.4     1.0       0.0  \n",
       "4                     40.0  3.51      0.56      9.4     1.0       0.0  \n",
       "...                    ...   ...       ...      ...     ...       ...  \n",
       "5315                  92.0  3.27      0.50     11.2     0.0       1.0  \n",
       "5316                 168.0  3.15      0.46      9.6     0.0       1.0  \n",
       "5317                 111.0  2.99      0.46      9.4     0.0       1.0  \n",
       "5318                 110.0  3.34      0.38     12.8     0.0       1.0  \n",
       "5319                  98.0  3.26      0.32     11.8     0.0       1.0  \n",
       "\n",
       "[5320 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make new df of un-split chosen features only, outliers removed\n",
    "wines_dropped_no_outliers = wines_clean.drop(['index', 'quality', 'color', 'color_cat', 'free_sulfur_dioxide','density', 'quality_label'], axis = 1)\n",
    "\n",
    "#make new x and y data\n",
    "y_data_dno = wines_dropped_no_outliers['quality_label_cat']\n",
    "x_data_dno = wines_dropped_no_outliers.drop(['quality_label_cat'], axis = 1) \n",
    "\n",
    "display(x_data_dno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Fold 1</th>\n",
       "      <th>Fold 2</th>\n",
       "      <th>Fold 3</th>\n",
       "      <th>Fold 4</th>\n",
       "      <th>Fold 5</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.712406</td>\n",
       "      <td>0.673872</td>\n",
       "      <td>0.682331</td>\n",
       "      <td>0.725564</td>\n",
       "      <td>0.760338</td>\n",
       "      <td>0.710902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.708647</td>\n",
       "      <td>0.672932</td>\n",
       "      <td>0.674812</td>\n",
       "      <td>0.728383</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.709586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.705827</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.659774</td>\n",
       "      <td>0.730263</td>\n",
       "      <td>0.755639</td>\n",
       "      <td>0.702444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.566729</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>0.595865</td>\n",
       "      <td>0.583647</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.593985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.568609</td>\n",
       "      <td>0.588346</td>\n",
       "      <td>0.594925</td>\n",
       "      <td>0.647556</td>\n",
       "      <td>0.667293</td>\n",
       "      <td>0.613346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>0.641917</td>\n",
       "      <td>0.355263</td>\n",
       "      <td>0.601504</td>\n",
       "      <td>0.505639</td>\n",
       "      <td>0.713346</td>\n",
       "      <td>0.563534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.466165</td>\n",
       "      <td>0.637218</td>\n",
       "      <td>0.612782</td>\n",
       "      <td>0.655075</td>\n",
       "      <td>0.540414</td>\n",
       "      <td>0.582331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.718045</td>\n",
       "      <td>0.671992</td>\n",
       "      <td>0.673872</td>\n",
       "      <td>0.724624</td>\n",
       "      <td>0.765977</td>\n",
       "      <td>0.710902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.693609</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.703947</td>\n",
       "      <td>0.720865</td>\n",
       "      <td>0.741541</td>\n",
       "      <td>0.708835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.670113</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.612782</td>\n",
       "      <td>0.690789</td>\n",
       "      <td>0.780075</td>\n",
       "      <td>0.682331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model    Fold 1    Fold 2  \\\n",
       "0                  LogisticRegression(max_iter=3000)  0.712406  0.673872   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...  0.708647  0.672932   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')  0.705827  0.660714   \n",
       "3                           DecisionTreeClassifier()  0.566729  0.592105   \n",
       "4                             KNeighborsClassifier()  0.568609  0.588346   \n",
       "5                           LinearSVC(max_iter=3000)  0.641917  0.355263   \n",
       "6                                       GaussianNB()  0.466165  0.637218   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)  0.718045  0.671992   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...  0.693609  0.684211   \n",
       "9                                    MLPClassifier()  0.670113  0.657895   \n",
       "\n",
       "     Fold 3    Fold 4    Fold 5  Average Accuracy  \n",
       "0  0.682331  0.725564  0.760338          0.710902  \n",
       "1  0.674812  0.728383  0.763158          0.709586  \n",
       "2  0.659774  0.730263  0.755639          0.702444  \n",
       "3  0.595865  0.583647  0.631579          0.593985  \n",
       "4  0.594925  0.647556  0.667293          0.613346  \n",
       "5  0.601504  0.505639  0.713346          0.563534  \n",
       "6  0.612782  0.655075  0.540414          0.582331  \n",
       "7  0.673872  0.724624  0.765977          0.710902  \n",
       "8  0.703947  0.720865  0.741541          0.708835  \n",
       "9  0.612782  0.690789  0.780075          0.682331  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#use the scikit-learn method cross_val_score (NB- not the same as cross_validate)\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "#set up a K-Fold with 5 folds\n",
    "kf = KFold(n_splits=5, random_state=None)\n",
    "\n",
    "#make a df to hold the fold scores and average\n",
    "kfold_scores = pd.DataFrame(columns=['Model', 'Fold 1','Fold 2','Fold 3','Fold 4','Fold 5', 'Average Accuracy'])\n",
    "\n",
    "#loop over models, use data with dropped features (x_data1) and RobustScaler for Standardisation- ### TODO: is this scaling in the right order, or should it be done after fold split?\n",
    "for model in classifier_model_defs:\n",
    "    result = cross_val_score(model, x_data_dno, y_data_dno, cv = kf)  \n",
    "    kfold_scores.loc[len(kfold_scores)] = [model, result[0], result[1], result[2], result[3], result[4], result.mean()]\n",
    "    #print(model, \"Per fold:\".format(result), \"Avg accuracy: {:.4f}\".format(result.mean()))\n",
    "\n",
    "display(kfold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pipeline to apply normalisation after K-fold splits, using RandomForest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because sklearn's K-Fold splits the data for us, we cannot easily apply standardisation or nomalisation to each of the folds. Doind it before splitting is not advised becasue of data leakage. To do this, I need to use pipeline. (This will also be needed for gridsearch later.)\n",
    "\n",
    "Pipeline is a list of transformations and callable names (as tuples), always ending with am estimator. make_pipeline allows us to omit the names.\n",
    "\n",
    "The pipeline can then be called used in place of the model in fit() and predict().\n",
    "\n",
    "Try out pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "#set up pipeline with robustscaler and randomforest\n",
    "pipeline = Pipeline(steps=[('scaler', RobustScaler()),\n",
    "                   ('model', RandomForestClassifier(random_state=42))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.7232063361763738\n"
     ]
    }
   ],
   "source": [
    "#attempt 1, try using pipeline only, without kfold\n",
    "\n",
    "#first, get new train test data as not using kfold\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data_dno, y_data_dno, test_size=0.25, random_state=42)\n",
    "\n",
    "#use pipeline to fit and predict\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "#show F1 score\n",
    "score = f1_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "print(f'F1-score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68890977 0.68327068 0.69454887 0.71428571 0.75      ] mean:  0.706203007518797\n"
     ]
    }
   ],
   "source": [
    "#attempt 3, pipeline and kfold https://stackoverflow.com/questions/44446501/how-to-standardize-data-with-sklearns-cross-val-score\n",
    "scores = cross_val_score(pipeline, x_data_dno, y_data_dno, cv = kf) #remember, kf is the kfold variable made above\n",
    "print(scores, 'mean: ', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler/Normaliser</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.712030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>MaxAbsScaler()</td>\n",
       "      <td>0.708835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.712970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>0.712406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>0.585714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.711842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>0.712030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>0.645865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>QuantileTransformer()</td>\n",
       "      <td>0.717857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>PowerTransformer()</td>\n",
       "      <td>0.704135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model      Scaler/Normaliser  Average Accuracy\n",
       "0   LogisticRegression(max_iter=3000)         MinMaxScaler()          0.712030\n",
       "1   LogisticRegression(max_iter=3000)         MaxAbsScaler()          0.708835\n",
       "2   LogisticRegression(max_iter=3000)       StandardScaler()          0.712970\n",
       "3   LogisticRegression(max_iter=3000)         RobustScaler()          0.712406\n",
       "4   LogisticRegression(max_iter=3000)           Normalizer()          0.585714\n",
       "..                                ...                    ...               ...\n",
       "65                    MLPClassifier()       StandardScaler()          0.711842\n",
       "66                    MLPClassifier()         RobustScaler()          0.712030\n",
       "67                    MLPClassifier()           Normalizer()          0.645865\n",
       "68                    MLPClassifier()  QuantileTransformer()          0.717857\n",
       "69                    MLPClassifier()     PowerTransformer()          0.704135\n",
       "\n",
       "[70 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#attempt 4, put the pipeline in the loop and try various scalers on various models\n",
    "                       \n",
    "#make a df for scaled data to hold the scores\n",
    "kfold_scores_scaled = pd.DataFrame(columns=['Model', 'Scaler/Normaliser', 'Average Accuracy'])\n",
    "from sklearn.preprocessing import MaxAbsScaler, QuantileTransformer, PowerTransformer, Normalizer\n",
    "\n",
    "#list scalers\n",
    "scaler_list = [MinMaxScaler(), MaxAbsScaler(),StandardScaler(), RobustScaler(), Normalizer(), QuantileTransformer(), PowerTransformer()]\n",
    "\n",
    "#nested loop over models/scalers\n",
    "for model in classifier_model_defs:\n",
    "    for scaler in scaler_list:\n",
    "        pipeline2 = make_pipeline(scaler, model)\n",
    "        result = cross_val_score(pipeline2, x_data_dno, y_data_dno, cv = kf)  \n",
    "        kfold_scores_scaled.loc[len(kfold_scores_scaled)] = [model, scaler, result.mean()]\n",
    "        #print(model, scaler, \"Per fold:\".format(result[0]), \"Avg accuracy: {:.4f}\".format(result.mean()))\n",
    "\n",
    "display(kfold_scores_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler/Normaliser</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>QuantileTransformer()</td>\n",
       "      <td>0.717857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.715977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>MaxAbsScaler()</td>\n",
       "      <td>0.715038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>PowerTransformer()</td>\n",
       "      <td>0.713722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>MaxAbsScaler()</td>\n",
       "      <td>0.713722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>PowerTransformer()</td>\n",
       "      <td>0.713722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>QuantileTransformer()</td>\n",
       "      <td>0.713534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>0.713534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.713346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>PowerTransformer()</td>\n",
       "      <td>0.713158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>PowerTransformer()</td>\n",
       "      <td>0.713158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.712970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.712970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>QuantileTransformer()</td>\n",
       "      <td>0.712594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>0.712406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>0.712406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>0.712030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.712030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.712030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.711842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>QuantileTransformer()</td>\n",
       "      <td>0.711654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>QuantileTransformer()</td>\n",
       "      <td>0.711654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>QuantileTransformer()</td>\n",
       "      <td>0.711654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.711466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>PowerTransformer()</td>\n",
       "      <td>0.711466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>0.710902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.710902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>MaxAbsScaler()</td>\n",
       "      <td>0.710902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.710902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>MaxAbsScaler()</td>\n",
       "      <td>0.709962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>PowerTransformer()</td>\n",
       "      <td>0.709962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>QuantileTransformer()</td>\n",
       "      <td>0.709962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>0.709211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.709211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.709211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>MaxAbsScaler()</td>\n",
       "      <td>0.708835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.708835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>MaxAbsScaler()</td>\n",
       "      <td>0.708835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>MaxAbsScaler()</td>\n",
       "      <td>0.707707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>0.706579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.704135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>PowerTransformer()</td>\n",
       "      <td>0.704135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>0.698308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>PowerTransformer()</td>\n",
       "      <td>0.676692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.674436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>QuantileTransformer()</td>\n",
       "      <td>0.673120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>MaxAbsScaler()</td>\n",
       "      <td>0.667105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.666165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>0.659774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>0.651692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>0.645865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>QuantileTransformer()</td>\n",
       "      <td>0.622180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>PowerTransformer()</td>\n",
       "      <td>0.608647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>0.604699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>0.599248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>0.593797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>0.591541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>MaxAbsScaler()</td>\n",
       "      <td>0.587218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.586654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>0.585714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.585714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>0.585714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>LinearSVC(max_iter=3000)</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>0.584962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>QuantileTransformer()</td>\n",
       "      <td>0.584398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>PowerTransformer()</td>\n",
       "      <td>0.582895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>MaxAbsScaler()</td>\n",
       "      <td>0.582331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>0.582331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>0.582331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>0.582331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>0.566917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Model      Scaler/Normaliser  \\\n",
       "68                                    MLPClassifier()  QuantileTransformer()   \n",
       "63                                    MLPClassifier()         MinMaxScaler()   \n",
       "64                                    MLPClassifier()         MaxAbsScaler()   \n",
       "41                           LinearSVC(max_iter=3000)     PowerTransformer()   \n",
       "57  (DecisionTreeClassifier(max_features='sqrt', r...         MaxAbsScaler()   \n",
       "13  LogisticRegression(max_iter=3000, solver='libl...     PowerTransformer()   \n",
       "61  (DecisionTreeClassifier(max_features='sqrt', r...  QuantileTransformer()   \n",
       "10  LogisticRegression(max_iter=3000, solver='libl...         RobustScaler()   \n",
       "9   LogisticRegression(max_iter=3000, solver='libl...       StandardScaler()   \n",
       "20   LogisticRegression(max_iter=3000, solver='saga')     PowerTransformer()   \n",
       "6                   LogisticRegression(max_iter=3000)     PowerTransformer()   \n",
       "16   LogisticRegression(max_iter=3000, solver='saga')       StandardScaler()   \n",
       "2                   LogisticRegression(max_iter=3000)       StandardScaler()   \n",
       "12  LogisticRegression(max_iter=3000, solver='libl...  QuantileTransformer()   \n",
       "17   LogisticRegression(max_iter=3000, solver='saga')         RobustScaler()   \n",
       "3                   LogisticRegression(max_iter=3000)         RobustScaler()   \n",
       "66                                    MLPClassifier()         RobustScaler()   \n",
       "0                   LogisticRegression(max_iter=3000)         MinMaxScaler()   \n",
       "14   LogisticRegression(max_iter=3000, solver='saga')         MinMaxScaler()   \n",
       "65                                    MLPClassifier()       StandardScaler()   \n",
       "19   LogisticRegression(max_iter=3000, solver='saga')  QuantileTransformer()   \n",
       "5                   LogisticRegression(max_iter=3000)  QuantileTransformer()   \n",
       "40                           LinearSVC(max_iter=3000)  QuantileTransformer()   \n",
       "7   LogisticRegression(max_iter=3000, solver='libl...         MinMaxScaler()   \n",
       "55         LinearDiscriminantAnalysis(n_components=1)     PowerTransformer()   \n",
       "52         LinearDiscriminantAnalysis(n_components=1)         RobustScaler()   \n",
       "51         LinearDiscriminantAnalysis(n_components=1)       StandardScaler()   \n",
       "50         LinearDiscriminantAnalysis(n_components=1)         MaxAbsScaler()   \n",
       "49         LinearDiscriminantAnalysis(n_components=1)         MinMaxScaler()   \n",
       "36                           LinearSVC(max_iter=3000)         MaxAbsScaler()   \n",
       "62  (DecisionTreeClassifier(max_features='sqrt', r...     PowerTransformer()   \n",
       "54         LinearDiscriminantAnalysis(n_components=1)  QuantileTransformer()   \n",
       "38                           LinearSVC(max_iter=3000)         RobustScaler()   \n",
       "37                           LinearSVC(max_iter=3000)       StandardScaler()   \n",
       "35                           LinearSVC(max_iter=3000)         MinMaxScaler()   \n",
       "1                   LogisticRegression(max_iter=3000)         MaxAbsScaler()   \n",
       "56  (DecisionTreeClassifier(max_features='sqrt', r...         MinMaxScaler()   \n",
       "15   LogisticRegression(max_iter=3000, solver='saga')         MaxAbsScaler()   \n",
       "8   LogisticRegression(max_iter=3000, solver='libl...         MaxAbsScaler()   \n",
       "59  (DecisionTreeClassifier(max_features='sqrt', r...         RobustScaler()   \n",
       "58  (DecisionTreeClassifier(max_features='sqrt', r...       StandardScaler()   \n",
       "69                                    MLPClassifier()     PowerTransformer()   \n",
       "60  (DecisionTreeClassifier(max_features='sqrt', r...           Normalizer()   \n",
       "34                             KNeighborsClassifier()     PowerTransformer()   \n",
       "28                             KNeighborsClassifier()         MinMaxScaler()   \n",
       "33                             KNeighborsClassifier()  QuantileTransformer()   \n",
       "29                             KNeighborsClassifier()         MaxAbsScaler()   \n",
       "30                             KNeighborsClassifier()       StandardScaler()   \n",
       "31                             KNeighborsClassifier()         RobustScaler()   \n",
       "53         LinearDiscriminantAnalysis(n_components=1)           Normalizer()   \n",
       "67                                    MLPClassifier()           Normalizer()   \n",
       "47                                       GaussianNB()  QuantileTransformer()   \n",
       "48                                       GaussianNB()     PowerTransformer()   \n",
       "32                             KNeighborsClassifier()           Normalizer()   \n",
       "25                           DecisionTreeClassifier()           Normalizer()   \n",
       "24                           DecisionTreeClassifier()         RobustScaler()   \n",
       "11  LogisticRegression(max_iter=3000, solver='libl...           Normalizer()   \n",
       "22                           DecisionTreeClassifier()         MaxAbsScaler()   \n",
       "21                           DecisionTreeClassifier()         MinMaxScaler()   \n",
       "18   LogisticRegression(max_iter=3000, solver='saga')           Normalizer()   \n",
       "23                           DecisionTreeClassifier()       StandardScaler()   \n",
       "4                   LogisticRegression(max_iter=3000)           Normalizer()   \n",
       "39                           LinearSVC(max_iter=3000)           Normalizer()   \n",
       "26                           DecisionTreeClassifier()  QuantileTransformer()   \n",
       "27                           DecisionTreeClassifier()     PowerTransformer()   \n",
       "43                                       GaussianNB()         MaxAbsScaler()   \n",
       "45                                       GaussianNB()         RobustScaler()   \n",
       "44                                       GaussianNB()       StandardScaler()   \n",
       "42                                       GaussianNB()         MinMaxScaler()   \n",
       "46                                       GaussianNB()           Normalizer()   \n",
       "\n",
       "    Average Accuracy  \n",
       "68          0.717857  \n",
       "63          0.715977  \n",
       "64          0.715038  \n",
       "41          0.713722  \n",
       "57          0.713722  \n",
       "13          0.713722  \n",
       "61          0.713534  \n",
       "10          0.713534  \n",
       "9           0.713346  \n",
       "20          0.713158  \n",
       "6           0.713158  \n",
       "16          0.712970  \n",
       "2           0.712970  \n",
       "12          0.712594  \n",
       "17          0.712406  \n",
       "3           0.712406  \n",
       "66          0.712030  \n",
       "0           0.712030  \n",
       "14          0.712030  \n",
       "65          0.711842  \n",
       "19          0.711654  \n",
       "5           0.711654  \n",
       "40          0.711654  \n",
       "7           0.711466  \n",
       "55          0.711466  \n",
       "52          0.710902  \n",
       "51          0.710902  \n",
       "50          0.710902  \n",
       "49          0.710902  \n",
       "36          0.709962  \n",
       "62          0.709962  \n",
       "54          0.709962  \n",
       "38          0.709211  \n",
       "37          0.709211  \n",
       "35          0.709211  \n",
       "1           0.708835  \n",
       "56          0.708835  \n",
       "15          0.708835  \n",
       "8           0.707707  \n",
       "59          0.706579  \n",
       "58          0.704135  \n",
       "69          0.704135  \n",
       "60          0.698308  \n",
       "34          0.676692  \n",
       "28          0.674436  \n",
       "33          0.673120  \n",
       "29          0.667105  \n",
       "30          0.666165  \n",
       "31          0.659774  \n",
       "53          0.651692  \n",
       "67          0.645865  \n",
       "47          0.622180  \n",
       "48          0.608647  \n",
       "32          0.604699  \n",
       "25          0.599248  \n",
       "24          0.593797  \n",
       "11          0.591541  \n",
       "22          0.587218  \n",
       "21          0.586654  \n",
       "18          0.585714  \n",
       "23          0.585714  \n",
       "4           0.585714  \n",
       "39          0.584962  \n",
       "26          0.584398  \n",
       "27          0.582895  \n",
       "43          0.582331  \n",
       "45          0.582331  \n",
       "44          0.582331  \n",
       "42          0.582331  \n",
       "46          0.566917  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display whole table in order of accuracy\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "display(kfold_scores_scaled.sort_values(by=['Average Accuracy'], ascending=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tuning MLP and RandomForest with GridSearchCV and RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automate the finding the best parameters using GridSearchCV and RandomizedSearchCV\n",
    "#these use cross-validation (CV) to estimate the model’s performance in a “parameter space” (range of possible values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
       "             param_grid={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                         &#x27;alpha&#x27;: [0.0001, 0.05],\n",
       "                         &#x27;hidden_layer_sizes&#x27;: [(10, 30, 10), (20,)],\n",
       "                         &#x27;learning_rate&#x27;: [&#x27;constant&#x27;, &#x27;adaptive&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
       "             param_grid={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                         &#x27;alpha&#x27;: [0.0001, 0.05],\n",
       "                         &#x27;hidden_layer_sizes&#x27;: [(10, 30, 10), (20,)],\n",
       "                         &#x27;learning_rate&#x27;: [&#x27;constant&#x27;, &#x27;adaptive&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=100)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=100)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
       "             param_grid={'activation': ['tanh', 'relu'],\n",
       "                         'alpha': [0.0001, 0.05],\n",
       "                         'hidden_layer_sizes': [(10, 30, 10), (20,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_gs = MLPClassifier(max_iter=100)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)\n",
    "clf.fit(X_train, y_train) \n",
    "\n",
    "print('Best parameters found:\\n', clf.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More evaluation metric tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Reports and Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: these are not appropriate evaluation metrics and/or they are not set up right. find better ones there i can see per-target, per-model\n",
    "\n",
    "# for model in classifier_model_defs:\n",
    "#     predictions = model.predict(x_test_data)\n",
    "#     #print(classification_report(y_test_data, predictions))\n",
    "#     #ConfusionMatrixDisplay(confusion_matrix(y_test_data, predictions)).plot(cmap='BuPu') #add model title"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ideas next:\n",
    "- loop with kfold to evaluate parameters? https://machinelearningmastery.com/random-forest-ensemble-in-python/\n",
    "- loop over different datasets with dropped features?\n",
    "- graph model performance?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs for presentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data with and without outliers and redundant features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(style='dark')\n",
    "wines_dropped_no_outliers.hist(bins=15, color='pink', edgecolor='black', linewidth=1.0, xlabelsize=8, ylabelsize=8, grid=False)\n",
    "\n",
    "plt.tight_layout(rect=(0, 0, 1.2, 1.2))\n",
    "\n",
    "rt = plt.suptitle('Cleaned Wines Data: Univariate Plots', x=0.65, y=1.25, fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.63      0.67       504\n",
      "           1       0.75      0.83      0.79       794\n",
      "           2       0.00      0.00      0.00        32\n",
      "\n",
      "    accuracy                           0.73      1330\n",
      "   macro avg       0.48      0.49      0.48      1330\n",
      "weighted avg       0.72      0.73      0.72      1330\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot(cmap='BuPu')\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
