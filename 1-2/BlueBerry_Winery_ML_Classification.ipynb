{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlueBerry Winery - Wine Quality Prediction with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop models to determine the quality of the wines produced based on their chemical composition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>4893</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>4894</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>4895</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>4896</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>4897</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  fixed acidity  volatile acidity  citric acid  \\\n",
       "0              0            7.4              0.70         0.00   \n",
       "1              1            7.8              0.88         0.00   \n",
       "2              2            7.8              0.76         0.04   \n",
       "3              3           11.2              0.28         0.56   \n",
       "4              4            7.4              0.70         0.00   \n",
       "...          ...            ...               ...          ...   \n",
       "6492        4893            6.2              0.21         0.29   \n",
       "6493        4894            6.6              0.32         0.36   \n",
       "6494        4895            6.5              0.24         0.19   \n",
       "6495        4896            5.5              0.29         0.30   \n",
       "6496        4897            6.0              0.21         0.38   \n",
       "\n",
       "      residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  \\\n",
       "0                1.9      0.076                 11.0                  34.0   \n",
       "1                2.6      0.098                 25.0                  67.0   \n",
       "2                2.3      0.092                 15.0                  54.0   \n",
       "3                1.9      0.075                 17.0                  60.0   \n",
       "4                1.9      0.076                 11.0                  34.0   \n",
       "...              ...        ...                  ...                   ...   \n",
       "6492             1.6      0.039                 24.0                  92.0   \n",
       "6493             8.0      0.047                 57.0                 168.0   \n",
       "6494             1.2      0.041                 30.0                 111.0   \n",
       "6495             1.1      0.022                 20.0                 110.0   \n",
       "6496             0.8      0.020                 22.0                  98.0   \n",
       "\n",
       "      density    pH  sulphates  alcohol  quality quality_label  color  \n",
       "0     0.99780  3.51       0.56      9.4        5           low    red  \n",
       "1     0.99680  3.20       0.68      9.8        5           low    red  \n",
       "2     0.99700  3.26       0.65      9.8        5           low    red  \n",
       "3     0.99800  3.16       0.58      9.8        6        medium    red  \n",
       "4     0.99780  3.51       0.56      9.4        5           low    red  \n",
       "...       ...   ...        ...      ...      ...           ...    ...  \n",
       "6492  0.99114  3.27       0.50     11.2        6        medium  white  \n",
       "6493  0.99490  3.15       0.46      9.6        5           low  white  \n",
       "6494  0.99254  2.99       0.46      9.4        6        medium  white  \n",
       "6495  0.98869  3.34       0.38     12.8        7        medium  white  \n",
       "6496  0.98941  3.26       0.32     11.8        6        medium  white  \n",
       "\n",
       "[6497 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "red_wine = pd.read_csv('winequality-red.csv', sep=';')\n",
    "white_wine = pd.read_csv('winequality-white.csv', sep=';')\n",
    "wines = pd.read_csv('wines_combined.csv')\n",
    "display(wines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Change column headings to snake_case and use IUPAC standard spelling ('sulfates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>4893</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>4894</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>4895</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>4896</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>4897</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  fixed acidity  volatile acidity  citric acid  \\\n",
       "0              0            7.4              0.70         0.00   \n",
       "1              1            7.8              0.88         0.00   \n",
       "2              2            7.8              0.76         0.04   \n",
       "3              3           11.2              0.28         0.56   \n",
       "4              4            7.4              0.70         0.00   \n",
       "...          ...            ...               ...          ...   \n",
       "6492        4893            6.2              0.21         0.29   \n",
       "6493        4894            6.6              0.32         0.36   \n",
       "6494        4895            6.5              0.24         0.19   \n",
       "6495        4896            5.5              0.29         0.30   \n",
       "6496        4897            6.0              0.21         0.38   \n",
       "\n",
       "      residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  \\\n",
       "0                1.9      0.076                 11.0                  34.0   \n",
       "1                2.6      0.098                 25.0                  67.0   \n",
       "2                2.3      0.092                 15.0                  54.0   \n",
       "3                1.9      0.075                 17.0                  60.0   \n",
       "4                1.9      0.076                 11.0                  34.0   \n",
       "...              ...        ...                  ...                   ...   \n",
       "6492             1.6      0.039                 24.0                  92.0   \n",
       "6493             8.0      0.047                 57.0                 168.0   \n",
       "6494             1.2      0.041                 30.0                 111.0   \n",
       "6495             1.1      0.022                 20.0                 110.0   \n",
       "6496             0.8      0.020                 22.0                  98.0   \n",
       "\n",
       "      density    pH  sulphates  alcohol  quality quality_label  color  \n",
       "0     0.99780  3.51       0.56      9.4        5           low    red  \n",
       "1     0.99680  3.20       0.68      9.8        5           low    red  \n",
       "2     0.99700  3.26       0.65      9.8        5           low    red  \n",
       "3     0.99800  3.16       0.58      9.8        6        medium    red  \n",
       "4     0.99780  3.51       0.56      9.4        5           low    red  \n",
       "...       ...   ...        ...      ...      ...           ...    ...  \n",
       "6492  0.99114  3.27       0.50     11.2        6        medium  white  \n",
       "6493  0.99490  3.15       0.46      9.6        5           low  white  \n",
       "6494  0.99254  2.99       0.46      9.4        6        medium  white  \n",
       "6495  0.98869  3.34       0.38     12.8        7        medium  white  \n",
       "6496  0.98941  3.26       0.32     11.8        6        medium  white  \n",
       "\n",
       "[6497 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>4893</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>4894</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>4895</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>4896</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>4897</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  fixed_acidity  volatile_acidity  citric_acid  \\\n",
       "0              0            7.4              0.70         0.00   \n",
       "1              1            7.8              0.88         0.00   \n",
       "2              2            7.8              0.76         0.04   \n",
       "3              3           11.2              0.28         0.56   \n",
       "4              4            7.4              0.70         0.00   \n",
       "...          ...            ...               ...          ...   \n",
       "6492        4893            6.2              0.21         0.29   \n",
       "6493        4894            6.6              0.32         0.36   \n",
       "6494        4895            6.5              0.24         0.19   \n",
       "6495        4896            5.5              0.29         0.30   \n",
       "6496        4897            6.0              0.21         0.38   \n",
       "\n",
       "      residual_sugar  chlorides  free_sulfur_dioxide  total_sulfur_dioxide  \\\n",
       "0                1.9      0.076                 11.0                  34.0   \n",
       "1                2.6      0.098                 25.0                  67.0   \n",
       "2                2.3      0.092                 15.0                  54.0   \n",
       "3                1.9      0.075                 17.0                  60.0   \n",
       "4                1.9      0.076                 11.0                  34.0   \n",
       "...              ...        ...                  ...                   ...   \n",
       "6492             1.6      0.039                 24.0                  92.0   \n",
       "6493             8.0      0.047                 57.0                 168.0   \n",
       "6494             1.2      0.041                 30.0                 111.0   \n",
       "6495             1.1      0.022                 20.0                 110.0   \n",
       "6496             0.8      0.020                 22.0                  98.0   \n",
       "\n",
       "      density    pH  sulfates  alcohol  quality quality_label  color  \n",
       "0     0.99780  3.51      0.56      9.4        5           low    red  \n",
       "1     0.99680  3.20      0.68      9.8        5           low    red  \n",
       "2     0.99700  3.26      0.65      9.8        5           low    red  \n",
       "3     0.99800  3.16      0.58      9.8        6        medium    red  \n",
       "4     0.99780  3.51      0.56      9.4        5           low    red  \n",
       "...       ...   ...       ...      ...      ...           ...    ...  \n",
       "6492  0.99114  3.27      0.50     11.2        6        medium  white  \n",
       "6493  0.99490  3.15      0.46      9.6        5           low  white  \n",
       "6494  0.99254  2.99      0.46      9.4        6        medium  white  \n",
       "6495  0.98869  3.34      0.38     12.8        7        medium  white  \n",
       "6496  0.98941  3.26      0.32     11.8        6        medium  white  \n",
       "\n",
       "[6497 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(wines)\n",
    "wines = wines.rename(columns={\"fixed acidity\": \"fixed_acidity\", \"volatile acidity\": \"volatile_acidity\", \"citric acid\": \"citric_acid\", \"residual sugar\" : \"residual_sugar\", \"free sulfur dioxide\" : \"free_sulfur_dioxide\", \"total sulfur dioxide\" : \"total_sulfur_dioxide\", \"sulphates\" : \"sulfates\"})\n",
    "display(wines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Drop old index values form red/white datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines = wines.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0               7.4              0.70         0.00             1.9      0.076   \n",
       "1               7.8              0.88         0.00             2.6      0.098   \n",
       "2               7.8              0.76         0.04             2.3      0.092   \n",
       "3              11.2              0.28         0.56             1.9      0.075   \n",
       "4               7.4              0.70         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "6492            6.2              0.21         0.29             1.6      0.039   \n",
       "6493            6.6              0.32         0.36             8.0      0.047   \n",
       "6494            6.5              0.24         0.19             1.2      0.041   \n",
       "6495            5.5              0.29         0.30             1.1      0.022   \n",
       "6496            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulfates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51      0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20      0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26      0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16      0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51      0.56   \n",
       "...                   ...                   ...      ...   ...       ...   \n",
       "6492                 24.0                  92.0  0.99114  3.27      0.50   \n",
       "6493                 57.0                 168.0  0.99490  3.15      0.46   \n",
       "6494                 30.0                 111.0  0.99254  2.99      0.46   \n",
       "6495                 20.0                 110.0  0.98869  3.34      0.38   \n",
       "6496                 22.0                  98.0  0.98941  3.26      0.32   \n",
       "\n",
       "      alcohol  quality quality_label  color  \n",
       "0         9.4        5           low    red  \n",
       "1         9.8        5           low    red  \n",
       "2         9.8        5           low    red  \n",
       "3         9.8        6        medium    red  \n",
       "4         9.4        5           low    red  \n",
       "...       ...      ...           ...    ...  \n",
       "6492     11.2        6        medium  white  \n",
       "6493      9.6        5           low  white  \n",
       "6494      9.4        6        medium  white  \n",
       "6495     12.8        7        medium  white  \n",
       "6496     11.8        6        medium  white  \n",
       "\n",
       "[6497 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(wines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Counts of duplicated items:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False    5320\n",
       "True     1177\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>6492</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>6493</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>6494</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>6495</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>6496</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5320 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  fixed_acidity  volatile_acidity  citric_acid  residual_sugar  \\\n",
       "0         1            7.8              0.88         0.00             2.6   \n",
       "1         2            7.8              0.76         0.04             2.3   \n",
       "2         3           11.2              0.28         0.56             1.9   \n",
       "3         4            7.4              0.70         0.00             1.9   \n",
       "4         5            7.4              0.66         0.00             1.8   \n",
       "...     ...            ...               ...          ...             ...   \n",
       "5315   6492            6.2              0.21         0.29             1.6   \n",
       "5316   6493            6.6              0.32         0.36             8.0   \n",
       "5317   6494            6.5              0.24         0.19             1.2   \n",
       "5318   6495            5.5              0.29         0.30             1.1   \n",
       "5319   6496            6.0              0.21         0.38             0.8   \n",
       "\n",
       "      chlorides  free_sulfur_dioxide  total_sulfur_dioxide  density    pH  \\\n",
       "0         0.098                 25.0                  67.0  0.99680  3.20   \n",
       "1         0.092                 15.0                  54.0  0.99700  3.26   \n",
       "2         0.075                 17.0                  60.0  0.99800  3.16   \n",
       "3         0.076                 11.0                  34.0  0.99780  3.51   \n",
       "4         0.075                 13.0                  40.0  0.99780  3.51   \n",
       "...         ...                  ...                   ...      ...   ...   \n",
       "5315      0.039                 24.0                  92.0  0.99114  3.27   \n",
       "5316      0.047                 57.0                 168.0  0.99490  3.15   \n",
       "5317      0.041                 30.0                 111.0  0.99254  2.99   \n",
       "5318      0.022                 20.0                 110.0  0.98869  3.34   \n",
       "5319      0.020                 22.0                  98.0  0.98941  3.26   \n",
       "\n",
       "      sulfates  alcohol  quality quality_label  color  \n",
       "0         0.68      9.8        5           low    red  \n",
       "1         0.65      9.8        5           low    red  \n",
       "2         0.58      9.8        6        medium    red  \n",
       "3         0.56      9.4        5           low    red  \n",
       "4         0.56      9.4        5           low    red  \n",
       "...        ...      ...      ...           ...    ...  \n",
       "5315      0.50     11.2        6        medium  white  \n",
       "5316      0.46      9.6        5           low  white  \n",
       "5317      0.46      9.4        6        medium  white  \n",
       "5318      0.38     12.8        7        medium  white  \n",
       "5319      0.32     11.8        6        medium  white  \n",
       "\n",
       "[5320 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, check for duplicates using duplicated() ## FIXED now that second index removed\n",
    "# use the value counts of duplicated() to show how many items are duplicated\n",
    "display('Counts of duplicated items:', wines.duplicated().value_counts())\n",
    "\n",
    "## workaround, no longer needed #############\n",
    "# Make a a new df contining only numerical data and no index (indexes are always unique and might cause a false negative)\n",
    "#df_dup_compare = wines.loc[:,'fixed acidity':'quality']\n",
    "\n",
    "# use the value counts of duplicated() to show how many items are duplicated\n",
    "#display(\"Counts of items that are duplications of previous items (in new df):\", df_dup_compare.duplicated().value_counts())\n",
    "\n",
    "# display all the rows of the duplicated() output series where duplicated==True\n",
    "## use keep=False to (counterintuitively) keep all instances of duplicated entries\n",
    "#display(\"All indices with dupicated data:\", df_dup_compare.duplicated(keep=False)[lambda x : x == True])\n",
    "\n",
    "# display all the rows of the df where duplicated == True\n",
    "#wines_duplicates = df_dup_compare[df_dup_compare.duplicated() == True]\n",
    "#display(\"All df rows which are duplications:\", wines_duplicates)\n",
    "\n",
    "# write csv of duplicated data for checking\n",
    "#wines_duplicates.to_csv('wines_duplicates.csv')\n",
    "#####################################\n",
    "\n",
    "wines_clean = wines.drop_duplicates(keep='last').reset_index()\n",
    "display(wines_clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Drop wines outside legal limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_sulfur_dioxide_max =  400  ## including â€˜colheita tardiaâ€™ https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32019R0934&rid=2\n",
    "## decided not to do this as only one wine falls into this, if indeed sweeter, late-harvest wines are included"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check and change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                     int64\n",
       "fixed_acidity           float64\n",
       "volatile_acidity        float64\n",
       "citric_acid             float64\n",
       "residual_sugar          float64\n",
       "chlorides               float64\n",
       "free_sulfur_dioxide     float64\n",
       "total_sulfur_dioxide    float64\n",
       "density                 float64\n",
       "pH                      float64\n",
       "sulfates                float64\n",
       "alcohol                 float64\n",
       "quality                   int64\n",
       "quality_label            object\n",
       "color                    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wines_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>6492</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>6493</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>6494</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>6495</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>6496</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5320 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  fixed_acidity  volatile_acidity  citric_acid  residual_sugar  \\\n",
       "0         1            7.8              0.88         0.00             2.6   \n",
       "1         2            7.8              0.76         0.04             2.3   \n",
       "2         3           11.2              0.28         0.56             1.9   \n",
       "3         4            7.4              0.70         0.00             1.9   \n",
       "4         5            7.4              0.66         0.00             1.8   \n",
       "...     ...            ...               ...          ...             ...   \n",
       "5315   6492            6.2              0.21         0.29             1.6   \n",
       "5316   6493            6.6              0.32         0.36             8.0   \n",
       "5317   6494            6.5              0.24         0.19             1.2   \n",
       "5318   6495            5.5              0.29         0.30             1.1   \n",
       "5319   6496            6.0              0.21         0.38             0.8   \n",
       "\n",
       "      chlorides  free_sulfur_dioxide  total_sulfur_dioxide  density    pH  \\\n",
       "0         0.098                 25.0                  67.0  0.99680  3.20   \n",
       "1         0.092                 15.0                  54.0  0.99700  3.26   \n",
       "2         0.075                 17.0                  60.0  0.99800  3.16   \n",
       "3         0.076                 11.0                  34.0  0.99780  3.51   \n",
       "4         0.075                 13.0                  40.0  0.99780  3.51   \n",
       "...         ...                  ...                   ...      ...   ...   \n",
       "5315      0.039                 24.0                  92.0  0.99114  3.27   \n",
       "5316      0.047                 57.0                 168.0  0.99490  3.15   \n",
       "5317      0.041                 30.0                 111.0  0.99254  2.99   \n",
       "5318      0.022                 20.0                 110.0  0.98869  3.34   \n",
       "5319      0.020                 22.0                  98.0  0.98941  3.26   \n",
       "\n",
       "      sulfates  alcohol  quality quality_label  color  \n",
       "0         0.68      9.8        5           low    red  \n",
       "1         0.65      9.8        5           low    red  \n",
       "2         0.58      9.8        6        medium    red  \n",
       "3         0.56      9.4        5           low    red  \n",
       "4         0.56      9.4        5           low    red  \n",
       "...        ...      ...      ...           ...    ...  \n",
       "5315      0.50     11.2        6        medium  white  \n",
       "5316      0.46      9.6        5           low  white  \n",
       "5317      0.46      9.4        6        medium  white  \n",
       "5318      0.38     12.8        7        medium  white  \n",
       "5319      0.32     11.8        6        medium  white  \n",
       "\n",
       "[5320 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cast columns to Categorical (pandas) data type:\n",
    "# color, unordered\n",
    "wines_clean['color'] = pd.Categorical(wines_clean['color'],\n",
    "ordered=False)\n",
    "\n",
    "# quality_label, ordered low to high\n",
    "wines_clean['quality_label'] = pd.Categorical(wines_clean['quality_label'],\n",
    "categories=['low', 'medium', 'high'], ordered=True)\n",
    "\n",
    "wines_clean.dtypes\n",
    "\n",
    "display(wines_clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate skewness and kurtosis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness: outside -0.5 and +0.5 is considered highly skewed [source?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index                   0.050375\n",
      "fixed_acidity           1.650417\n",
      "volatile_acidity        1.504557\n",
      "citric_acid             0.484309\n",
      "residual_sugar          1.706550\n",
      "chlorides               5.338237\n",
      "free_sulfur_dioxide     1.362719\n",
      "total_sulfur_dioxide    0.063614\n",
      "density                 0.666326\n",
      "pH                      0.389969\n",
      "sulfates                1.809454\n",
      "alcohol                 0.545696\n",
      "quality                 0.147467\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print((wines_clean.select_dtypes(include=['int64', 'float64'])).astype(float).skew())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurtosis: XXX is considered high [source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index                   -1.188777\n",
      "fixed_acidity            4.589079\n",
      "volatile_acidity         2.863175\n",
      "citric_acid              2.582471\n",
      "residual_sugar           7.025595\n",
      "chlorides               48.260708\n",
      "free_sulfur_dioxide      9.520706\n",
      "total_sulfur_dioxide    -0.299997\n",
      "density                  8.711498\n",
      "pH                       0.431811\n",
      "sulfates                 8.612917\n",
      "alcohol                 -0.538169\n",
      "quality                  0.298100\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print((wines_clean.select_dtypes(include=['int64', 'float64'])).astype(float).kurt())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing for ML 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Encoding categorical variables\n",
    "\n",
    "Use scikit. Resources: \n",
    "- https://scikit-learn.org/stable/modules/preprocessing.html \n",
    "- https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n",
    "- https://scikit-learn.org/stable/modules/preprocessing_targets.html#preprocessing-targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Label (Ordinal: Label Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "      <th>quality_label_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>6492</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>6493</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>6494</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>6495</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>6496</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5320 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  fixed_acidity  volatile_acidity  citric_acid  residual_sugar  \\\n",
       "0         1            7.8              0.88         0.00             2.6   \n",
       "1         2            7.8              0.76         0.04             2.3   \n",
       "2         3           11.2              0.28         0.56             1.9   \n",
       "3         4            7.4              0.70         0.00             1.9   \n",
       "4         5            7.4              0.66         0.00             1.8   \n",
       "...     ...            ...               ...          ...             ...   \n",
       "5315   6492            6.2              0.21         0.29             1.6   \n",
       "5316   6493            6.6              0.32         0.36             8.0   \n",
       "5317   6494            6.5              0.24         0.19             1.2   \n",
       "5318   6495            5.5              0.29         0.30             1.1   \n",
       "5319   6496            6.0              0.21         0.38             0.8   \n",
       "\n",
       "      chlorides  free_sulfur_dioxide  total_sulfur_dioxide  density    pH  \\\n",
       "0         0.098                 25.0                  67.0  0.99680  3.20   \n",
       "1         0.092                 15.0                  54.0  0.99700  3.26   \n",
       "2         0.075                 17.0                  60.0  0.99800  3.16   \n",
       "3         0.076                 11.0                  34.0  0.99780  3.51   \n",
       "4         0.075                 13.0                  40.0  0.99780  3.51   \n",
       "...         ...                  ...                   ...      ...   ...   \n",
       "5315      0.039                 24.0                  92.0  0.99114  3.27   \n",
       "5316      0.047                 57.0                 168.0  0.99490  3.15   \n",
       "5317      0.041                 30.0                 111.0  0.99254  2.99   \n",
       "5318      0.022                 20.0                 110.0  0.98869  3.34   \n",
       "5319      0.020                 22.0                  98.0  0.98941  3.26   \n",
       "\n",
       "      sulfates  alcohol  quality quality_label  color  quality_label_cat  \n",
       "0         0.68      9.8        5           low    red                  0  \n",
       "1         0.65      9.8        5           low    red                  0  \n",
       "2         0.58      9.8        6        medium    red                  1  \n",
       "3         0.56      9.4        5           low    red                  0  \n",
       "4         0.56      9.4        5           low    red                  0  \n",
       "...        ...      ...      ...           ...    ...                ...  \n",
       "5315      0.50     11.2        6        medium  white                  1  \n",
       "5316      0.46      9.6        5           low  white                  0  \n",
       "5317      0.46      9.4        6        medium  white                  1  \n",
       "5318      0.38     12.8        7        medium  white                  1  \n",
       "5319      0.32     11.8        6        medium  white                  1  \n",
       "\n",
       "[5320 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add a new cat_ variable storing a numerical code for each category\n",
    "## note: didn't use LabelEncoder, used .cat.codes as per https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n",
    "\n",
    "wines_clean['quality_label_cat'] = wines_clean['quality_label'].cat.codes \n",
    "display(wines_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3179\n",
       "0    1988\n",
       "2     153\n",
       "Name: quality_label_cat, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "medium    3179\n",
       "low       1988\n",
       "high       153\n",
       "Name: quality_label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(wines_clean.quality_label_cat.value_counts(), wines_clean.quality_label.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color (Nominal: One-Hot Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>color</th>\n",
       "      <th>quality_label_cat</th>\n",
       "      <th>color_cat</th>\n",
       "      <th>is_red</th>\n",
       "      <th>is_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>red</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>6492</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>6493</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>6494</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>6495</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>6496</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>medium</td>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5320 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  fixed_acidity  volatile_acidity  citric_acid  residual_sugar  \\\n",
       "0         1            7.8              0.88         0.00             2.6   \n",
       "1         2            7.8              0.76         0.04             2.3   \n",
       "2         3           11.2              0.28         0.56             1.9   \n",
       "3         4            7.4              0.70         0.00             1.9   \n",
       "4         5            7.4              0.66         0.00             1.8   \n",
       "...     ...            ...               ...          ...             ...   \n",
       "5315   6492            6.2              0.21         0.29             1.6   \n",
       "5316   6493            6.6              0.32         0.36             8.0   \n",
       "5317   6494            6.5              0.24         0.19             1.2   \n",
       "5318   6495            5.5              0.29         0.30             1.1   \n",
       "5319   6496            6.0              0.21         0.38             0.8   \n",
       "\n",
       "      chlorides  free_sulfur_dioxide  total_sulfur_dioxide  density    pH  \\\n",
       "0         0.098                 25.0                  67.0  0.99680  3.20   \n",
       "1         0.092                 15.0                  54.0  0.99700  3.26   \n",
       "2         0.075                 17.0                  60.0  0.99800  3.16   \n",
       "3         0.076                 11.0                  34.0  0.99780  3.51   \n",
       "4         0.075                 13.0                  40.0  0.99780  3.51   \n",
       "...         ...                  ...                   ...      ...   ...   \n",
       "5315      0.039                 24.0                  92.0  0.99114  3.27   \n",
       "5316      0.047                 57.0                 168.0  0.99490  3.15   \n",
       "5317      0.041                 30.0                 111.0  0.99254  2.99   \n",
       "5318      0.022                 20.0                 110.0  0.98869  3.34   \n",
       "5319      0.020                 22.0                  98.0  0.98941  3.26   \n",
       "\n",
       "      sulfates  alcohol  quality quality_label  color  quality_label_cat  \\\n",
       "0         0.68      9.8        5           low    red                  0   \n",
       "1         0.65      9.8        5           low    red                  0   \n",
       "2         0.58      9.8        6        medium    red                  1   \n",
       "3         0.56      9.4        5           low    red                  0   \n",
       "4         0.56      9.4        5           low    red                  0   \n",
       "...        ...      ...      ...           ...    ...                ...   \n",
       "5315      0.50     11.2        6        medium  white                  1   \n",
       "5316      0.46      9.6        5           low  white                  0   \n",
       "5317      0.46      9.4        6        medium  white                  1   \n",
       "5318      0.38     12.8        7        medium  white                  1   \n",
       "5319      0.32     11.8        6        medium  white                  1   \n",
       "\n",
       "      color_cat  is_red  is_white  \n",
       "0             0     1.0       0.0  \n",
       "1             0     1.0       0.0  \n",
       "2             0     1.0       0.0  \n",
       "3             0     1.0       0.0  \n",
       "4             0     1.0       0.0  \n",
       "...         ...     ...       ...  \n",
       "5315          1     0.0       1.0  \n",
       "5316          1     0.0       1.0  \n",
       "5317          1     0.0       1.0  \n",
       "5318          1     0.0       1.0  \n",
       "5319          1     0.0       1.0  \n",
       "\n",
       "[5320 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OHE creates new dummy variable columns for each category, with binary encoding (one of them 1, all others 0) indicating whether the category is applied\n",
    "\n",
    "# Step 1: OHE needs numerica data, so first transform categorical data using LabelEncoder() [This time really use LabelEncoder!]\n",
    "\n",
    "# create an instance of labelencoder  -TODO: explain this more\n",
    "labelencoder = LabelEncoder()\n",
    "# make new column and apply numerical category values\n",
    "wines_clean['color_cat'] = labelencoder.fit_transform(wines_clean['color'])\n",
    "\n",
    "# Step 2: Now use OneHotEncoder. reference: https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n",
    "## First, create an instance of one-hot-encoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# pass in color_cat to make a df containing the multiple (here, 2) binary encoding columns\n",
    "enc_df = pd.DataFrame(enc.fit_transform(wines_clean[['color_cat']]).toarray())\n",
    "#display('encoder array:', enc_df)\n",
    "\n",
    "# merge df with wines_clean\n",
    "wines_clean = wines_clean.join(enc_df)\n",
    "\n",
    "# rename 0 and 1 (this time manually but look into get_feature_names_out and ColumnTransformer)\n",
    "wines_clean = wines_clean.rename(columns={0: 'is_red', 1: 'is_white'})\n",
    "\n",
    "#drop extra index\n",
    "#wines_clean = wines_clean.drop('index', axis=1) TODO: drop index here instead of in every new set\n",
    "display(wines_clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Linear Regression Machine Learning Model with Single Variable: Density)\n",
    "\n",
    "\n",
    "note: not to be confused with logistical regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Separate off training set (split)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `test_train_split` to separate testing and traning portions of the data. [criteria for % proportion, source]\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99584],\n",
       "       [0.997  ],\n",
       "       [0.99212],\n",
       "       ...,\n",
       "       [0.9939 ],\n",
       "       [0.99415],\n",
       "       [0.9938 ]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## first split for single variable linear regression\n",
    "#make new variables for???\n",
    "density = wines_clean.density\n",
    "quality = wines_clean.quality\n",
    "\n",
    "#first reshape (which data?? workbook says main data but web says test data) into ndarray otherwise we get an error #TODO: expand (https://stackoverflow.com/questions/47761744/cant-do-linear-regression-in-scikit-learn-due-to-reshaping-issue)\n",
    "density = density.values.reshape(-1, 1)\n",
    "quality = quality.values.reshape(-1, 1)\n",
    "\n",
    "#split data\n",
    "density_train, density_test, quality_train, quality_test = train_test_split(density, quality, test_size=0.2, random_state=0)\n",
    "density_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_density = linear_model.LinearRegression()\n",
    "reg_density.fit(density_train, quality_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make predicitons and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the ML model based on Living Area is: 12.26 %\n",
      "A wine of density 1 may have a quality of: [[5.27384779]] and is 12.26% accurate\n"
     ]
    }
   ],
   "source": [
    "# apply the .predict() method to make predictions using test data.\n",
    "#find accuracy score\n",
    "reg_density_score = (reg_density.score(density_test, quality_test) * 100).round(2)\n",
    "print(\"The accuracy of the ML model based on Living Area is:\", reg_density_score , \"%\")\n",
    "\n",
    "#pick a value to test prediction\n",
    "density_input = 1     ## variable to test y and X\n",
    "pred_1 = reg_density.predict([[density_input]]) \n",
    "print(f\"A wine of density {density_input} may have a quality of: {pred_1} and is {reg_density_score}% accurate\" )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.freecodecamp.org/news/how-to-build-and-train-linear-and-logistic-regression-ml-models-in-python/\n",
    "\n",
    "#define dfs for x and y\n",
    "y_data = wines_clean['quality_label_cat']\n",
    "x_data = wines_clean.drop(['index', 'quality_label_cat', 'color_cat', 'quality_label', 'quality', 'color'], axis = 1) \n",
    "\n",
    "#split training data\n",
    "x_training_data, x_test_data, y_training_data, y_test_data = train_test_split(x_data, y_data, test_size = 0.2, random_state=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>is_red</th>\n",
       "      <th>is_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5320 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0               7.8              0.88         0.00             2.6      0.098   \n",
       "1               7.8              0.76         0.04             2.3      0.092   \n",
       "2              11.2              0.28         0.56             1.9      0.075   \n",
       "3               7.4              0.70         0.00             1.9      0.076   \n",
       "4               7.4              0.66         0.00             1.8      0.075   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "5315            6.2              0.21         0.29             1.6      0.039   \n",
       "5316            6.6              0.32         0.36             8.0      0.047   \n",
       "5317            6.5              0.24         0.19             1.2      0.041   \n",
       "5318            5.5              0.29         0.30             1.1      0.022   \n",
       "5319            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulfates  \\\n",
       "0                    25.0                  67.0  0.99680  3.20      0.68   \n",
       "1                    15.0                  54.0  0.99700  3.26      0.65   \n",
       "2                    17.0                  60.0  0.99800  3.16      0.58   \n",
       "3                    11.0                  34.0  0.99780  3.51      0.56   \n",
       "4                    13.0                  40.0  0.99780  3.51      0.56   \n",
       "...                   ...                   ...      ...   ...       ...   \n",
       "5315                 24.0                  92.0  0.99114  3.27      0.50   \n",
       "5316                 57.0                 168.0  0.99490  3.15      0.46   \n",
       "5317                 30.0                 111.0  0.99254  2.99      0.46   \n",
       "5318                 20.0                 110.0  0.98869  3.34      0.38   \n",
       "5319                 22.0                  98.0  0.98941  3.26      0.32   \n",
       "\n",
       "      alcohol  is_red  is_white  \n",
       "0         9.8     1.0       0.0  \n",
       "1         9.8     1.0       0.0  \n",
       "2         9.8     1.0       0.0  \n",
       "3         9.4     1.0       0.0  \n",
       "4         9.4     1.0       0.0  \n",
       "...       ...     ...       ...  \n",
       "5315     11.2     0.0       1.0  \n",
       "5316      9.6     0.0       1.0  \n",
       "5317      9.4     0.0       1.0  \n",
       "5318     12.8     0.0       1.0  \n",
       "5319     11.8     0.0       1.0  \n",
       "\n",
       "[5320 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "5315    1\n",
       "5316    0\n",
       "5317    1\n",
       "5318    1\n",
       "5319    1\n",
       "Name: quality_label_cat, Length: 5320, dtype: int8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x_data)\n",
    "display(y_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=5000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=5000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=5000)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "#first, instantiate the model (create an instance of it with a name and any necessary parameters)\n",
    "lr = LogisticRegression(max_iter=5000)\n",
    "\n",
    "#then, use fit() to pass in the x and y training data\n",
    "lr.fit(x_training_data, y_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make predicitons and evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.61      0.64       383\n",
      "           1       0.75      0.81      0.78       651\n",
      "           2       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.72      1064\n",
      "   macro avg       0.47      0.48      0.47      1064\n",
      "weighted avg       0.70      0.72      0.71      1064\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions = lr.predict(x_test_data)\n",
    "print(classification_report(y_test_data, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Confusion Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification models. Returns a table of true/false - negative/positive predictions made by the model. It is not an evaluaiton metric but gives an overview. Not suitable for unbalanced data (which ours is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f1094524cd0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test_data, predictions)).plot(cmap='BuPu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Accuracy Score (not useful with imbalanced classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7189849624060151"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_data, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Cohen-Kappa Score (better for imbalanced classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A measure of classifier performance compared to a random-guessing model, especially good for an imbalanced data set (0-1, higher is better)\n",
    "\n",
    "\n",
    "Resources:\n",
    "- https://analyticsindiamag.com/understanding-cohens-kappa-score-with-hands-on-implementation/\n",
    "- https://www.knime.com/blog/cohens-kappa-an-overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4051753795527634"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(y_test_data, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: Moderate agreement = 0.40 to 0.60"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple ML models in a function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import more modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score, precision_score, roc_auc_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "### from unsuccessful attempts to read exception (scikit warning) message to write to results ##\n",
    "import warnings\n",
    "import logging\n",
    "import traceback\n",
    "import sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function cycles through a list of models and runs them on x and y training and testing data inputs.\n",
    "- Logistic Regression (with various solvers)\n",
    "- Decision Tree Classifier\n",
    "- K-Nearest Neighbors Classifier (does not use a training set)\n",
    "- Support Vector Machine [LinearSVC()]\n",
    "- Naive Bayes [GaussianNB()]\n",
    "- Linear Discriminant Analysis\n",
    "\n",
    "It outputs the models' scores on various tests:\n",
    "- Accuracy Score - the percentage of predictions that were correct (not good for imbalanced data)\n",
    "- Cohen Kappa - \n",
    "- Precision - correct positives\n",
    "- F1 Score - score combining precision (correct positives) and recall (sensitivity)\n",
    "- maybe (roc_auc_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "      <th>Warning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.410257</td>\n",
       "      <td>0.698301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.708739</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.726504</td>\n",
       "      <td>0.417588</td>\n",
       "      <td>0.702693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712709</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.707707</td>\n",
       "      <td>0.378500</td>\n",
       "      <td>0.683783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.694102</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.618421</td>\n",
       "      <td>0.239125</td>\n",
       "      <td>0.623144</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.620563</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.634398</td>\n",
       "      <td>0.223531</td>\n",
       "      <td>0.610447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620984</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC()</td>\n",
       "      <td>0.628759</td>\n",
       "      <td>0.058513</td>\n",
       "      <td>0.685953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.507477</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.586466</td>\n",
       "      <td>0.191420</td>\n",
       "      <td>0.607885</td>\n",
       "      <td>0.089744</td>\n",
       "      <td>0.595605</td>\n",
       "      <td>0.12963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.720865</td>\n",
       "      <td>0.407968</td>\n",
       "      <td>0.697315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707721</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.745301</td>\n",
       "      <td>0.464452</td>\n",
       "      <td>0.723057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.733521</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.731203</td>\n",
       "      <td>0.423819</td>\n",
       "      <td>0.707293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.716307</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.721805   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.726504   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.707707   \n",
       "3                           DecisionTreeClassifier()        0.618421   \n",
       "4                             KNeighborsClassifier()        0.634398   \n",
       "5                                        LinearSVC()        0.628759   \n",
       "6                                       GaussianNB()        0.586466   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.720865   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.745301   \n",
       "9                                    MLPClassifier()        0.731203   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  Warning  \n",
       "0     0.410257   0.698301         0.000000  0.708739         0.00000        0  \n",
       "1     0.417588   0.702693         0.000000  0.712709         0.00000        0  \n",
       "2     0.378500   0.683783         0.000000  0.694102         0.00000        0  \n",
       "3     0.239125   0.623144         0.117647  0.620563         0.12500        0  \n",
       "4     0.223531   0.610447         0.000000  0.620984         0.00000        0  \n",
       "5     0.058513   0.685953         0.000000  0.507477         0.00000        0  \n",
       "6     0.191420   0.607885         0.089744  0.595605         0.12963        0  \n",
       "7     0.407968   0.697315         0.000000  0.707721         0.00000        0  \n",
       "8     0.464452   0.723057         0.000000  0.733521         0.00000        0  \n",
       "9     0.423819   0.707293         0.000000  0.716307         0.00000        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#instantiate all models with any parameters needed\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=3000)  #NB: 'lbfgs' is the default solver. it failed to converge, so trying various solvers\n",
    "lr1 = LogisticRegression(solver='liblinear', max_iter=3000)\n",
    "lr2 = LogisticRegression(solver='newton-cg', max_iter=3000)\n",
    "lr3 = LogisticRegression(solver='newton-cholesky', max_iter=3000)\n",
    "lr4 = LogisticRegression(solver='sag', max_iter=3000)   #NB: sag requires normalisation to work\n",
    "lr5 = LogisticRegression(solver='saga', max_iter=3000)  #NB:saga requires normalisation\n",
    "dt = DecisionTreeClassifier()\n",
    "kn = KNeighborsClassifier() ## guessing this is the one for classification. https://www.datacamp.com/tutorial/k-nearest-neighbor-classification-scikit-learn\n",
    "ls = LinearSVC(dual = True)\n",
    "nb = GaussianNB()\n",
    "ld = LDA(n_components=1)\n",
    "rf = RandomForestClassifier()\n",
    "mp = MLPClassifier()\n",
    "#TODO: nn = sklearn.neural_network.MLPClassifier ? and others?\n",
    "\n",
    "#create list of all model instances\n",
    "classifier_model_defs = [lr, lr1, lr5, dt, kn, ls, nb, ld, rf, mp] ## not used: lr2, lr3, lr4,\n",
    "\n",
    "#define function\n",
    "def deploy_models(x_training_data, x_test_data, y_training_data, y_test_data): #feed in all inputs needed by models here    \n",
    "    df_models_eval = pd.DataFrame(columns=['Model', 'Accuracy Score', 'Cohen Kappa', 'Precision', 'Precision: high', 'F1 Score', 'F1 Score: high','Warning']) #make a df to hold the scores\n",
    "    for model in classifier_model_defs:             #loop over models in list\n",
    "        model.fit(x_training_data, y_training_data) #use fit() to train each model on the x and y training data\n",
    "        pred = model.predict(x_test_data)           #predict, evaluate and output some things to compare...\n",
    "        acc = accuracy_score(y_test_data, pred)\n",
    "        kap = cohen_kappa_score(y_test_data, pred)\n",
    "        prc = precision_score(y_test_data, pred, average='weighted', zero_division=0)\n",
    "        prc2 = precision_score(y_test_data, pred, average='weighted', zero_division=0, labels=[2]) \n",
    "        f1 = f1_score(y_test_data, pred, average='weighted', zero_division=0) # , labels=[2] to just show high quality\n",
    "        f12 = f1_score(y_test_data, pred, labels=[2], average='weighted', zero_division=0)\n",
    "        warn = 0 ## TODO: want to add error type so i can see which ones did not complete (ie Converge, so far)\n",
    "        df_models_eval.loc[len(df_models_eval)] = [model, acc, kap, prc, prc2, f1, f12, warn] #add row to the results df\n",
    "        #print('\\n', model, 'accuracy:', acc, 'kappa:', kap, 'f1:', f1) #test:print result per iteration (for when loop fails to finish and display df)\n",
    "    return(df_models_eval)\n",
    "\n",
    "#call function and save un-engineered test results to variable for comparison\n",
    "orig_results = deploy_models(x_training_data, x_test_data, y_training_data, y_test_data)\n",
    "\n",
    "display(orig_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "      <th>Warning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.981203</td>\n",
       "      <td>0.950847</td>\n",
       "      <td>0.981150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.981136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.978383</td>\n",
       "      <td>0.943005</td>\n",
       "      <td>0.978427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.978212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.888487</td>\n",
       "      <td>0.957409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.991541</td>\n",
       "      <td>0.978062</td>\n",
       "      <td>0.991553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.991546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.945489</td>\n",
       "      <td>0.855758</td>\n",
       "      <td>0.944972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.944952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC()</td>\n",
       "      <td>0.960526</td>\n",
       "      <td>0.892208</td>\n",
       "      <td>0.962526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.959420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.969925</td>\n",
       "      <td>0.922811</td>\n",
       "      <td>0.970479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.970095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.995301</td>\n",
       "      <td>0.987784</td>\n",
       "      <td>0.995298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.996241</td>\n",
       "      <td>0.990193</td>\n",
       "      <td>0.996260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.980263</td>\n",
       "      <td>0.948207</td>\n",
       "      <td>0.980233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.980156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.981203   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.978383   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.957707   \n",
       "3                           DecisionTreeClassifier()        0.991541   \n",
       "4                             KNeighborsClassifier()        0.945489   \n",
       "5                                        LinearSVC()        0.960526   \n",
       "6                                       GaussianNB()        0.969925   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.995301   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.996241   \n",
       "9                                    MLPClassifier()        0.980263   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  Warning  \n",
       "0     0.950847   0.981150              0.0  0.981136             0.0        0  \n",
       "1     0.943005   0.978427              0.0  0.978212             0.0        0  \n",
       "2     0.888487   0.957409              0.0  0.957372             0.0        0  \n",
       "3     0.978062   0.991553              0.0  0.991546             0.0        0  \n",
       "4     0.855758   0.944972              0.0  0.944952             0.0        0  \n",
       "5     0.892208   0.962526              0.0  0.959420             0.0        0  \n",
       "6     0.922811   0.970479              0.0  0.970095             0.0        0  \n",
       "7     0.987784   0.995298              0.0  0.995298             0.0        0  \n",
       "8     0.990193   0.996260              0.0  0.996232             0.0        0  \n",
       "9     0.948207   0.980233              0.0  0.980156             0.0        0  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test models on color as well\n",
    "\n",
    "#define dfs for x and y\n",
    "y_data_col = wines_clean['color_cat']\n",
    "x_data_col = wines_clean.drop(['color_cat', 'quality_label', 'quality', 'color', 'index', 'is_red', 'is_white'], axis = 1) \n",
    "\n",
    "#split training data\n",
    "x_training_data_col, x_test_data_col, y_training_data_col, y_test_data_col = train_test_split(x_data_col, y_data_col, test_size = 0.2)\n",
    "\n",
    "deploy_models(x_training_data_col, x_test_data_col, y_training_data_col, y_test_data_col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Feature reduction (round 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: drop highly correlated features and drop features that do not help in the prediction.\n",
    "\n",
    "Resource: https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: -1.17419\n",
      "Feature: 1, Score: -7.56931\n",
      "Feature: 2, Score: 0.75192\n",
      "Feature: 3, Score: 0.10989\n",
      "Feature: 4, Score: -2.56429\n",
      "Feature: 5, Score: -0.04917\n",
      "Feature: 6, Score: 0.05651\n",
      "Feature: 7, Score: -0.26409\n",
      "Feature: 8, Score: -5.92097\n",
      "Feature: 9, Score: -5.71795\n",
      "Feature: 10, Score: 0.52627\n",
      "Feature: 11, Score: 0.26130\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get importance\n",
    "importance = lr.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    " print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests with removed (dropped) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove free_sulfur_dioxide, density (strongly correlated to other vars). remove pH and residual_sugar (weakly corellated with target)\n",
    "\n",
    "#define dfs for x and y\n",
    "y_data = wines_clean['quality_label_cat']\n",
    "x_data1 = wines_clean.drop(['quality_label_cat', 'pH', 'free_sulfur_dioxide', 'density', 'residual_sugar', 'color_cat', 'quality_label', 'quality', 'color', 'index'], axis = 1) \n",
    "\n",
    "#split training data\n",
    "x_training_data1, x_test_data1, y_training_data1, y_test_data1 = train_test_split(x_data1, y_data, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "      <th>Warning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.713346</td>\n",
       "      <td>0.391941</td>\n",
       "      <td>0.692313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.701243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.716165</td>\n",
       "      <td>0.395291</td>\n",
       "      <td>0.694852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.703288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.707707</td>\n",
       "      <td>0.368376</td>\n",
       "      <td>0.685882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.691807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.661654</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.661105</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.661251</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.679511</td>\n",
       "      <td>0.323057</td>\n",
       "      <td>0.659751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.668405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC()</td>\n",
       "      <td>0.624060</td>\n",
       "      <td>0.077190</td>\n",
       "      <td>0.604637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.601504</td>\n",
       "      <td>0.196243</td>\n",
       "      <td>0.626944</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.603413</td>\n",
       "      <td>0.144144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.717105</td>\n",
       "      <td>0.398753</td>\n",
       "      <td>0.695956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.725564</td>\n",
       "      <td>0.421494</td>\n",
       "      <td>0.705183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.669173</td>\n",
       "      <td>0.351532</td>\n",
       "      <td>0.677030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.713346   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.716165   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.707707   \n",
       "3                           DecisionTreeClassifier()        0.661654   \n",
       "4                             KNeighborsClassifier()        0.679511   \n",
       "5                                        LinearSVC()        0.624060   \n",
       "6                                       GaussianNB()        0.601504   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.717105   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.725564   \n",
       "9                                    MLPClassifier()        0.669173   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  Warning  \n",
       "0     0.391941   0.692313         0.000000  0.701243        0.000000        0  \n",
       "1     0.395291   0.694852         0.000000  0.703288        0.000000        0  \n",
       "2     0.368376   0.685882         0.000000  0.691807        0.000000        0  \n",
       "3     0.312057   0.661105         0.071429  0.661251        0.074074        0  \n",
       "4     0.323057   0.659751         0.000000  0.668405        0.000000        0  \n",
       "5     0.077190   0.604637         0.000000  0.530298        0.000000        0  \n",
       "6     0.196243   0.626944         0.094118  0.603413        0.144144        0  \n",
       "7     0.398753   0.695956         0.000000  0.704671        0.000000        0  \n",
       "8     0.421494   0.705183         0.000000  0.714475        0.000000        0  \n",
       "9     0.351532   0.677030         0.000000  0.664496        0.000000        0  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deploy_models(x_training_data1, x_test_data1, y_training_data1, y_test_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "      <th>Warning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.410257</td>\n",
       "      <td>0.698301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.708739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.726504</td>\n",
       "      <td>0.417588</td>\n",
       "      <td>0.702693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.707707</td>\n",
       "      <td>0.378500</td>\n",
       "      <td>0.683783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.694102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.632519</td>\n",
       "      <td>0.270194</td>\n",
       "      <td>0.639061</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.635487</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.634398</td>\n",
       "      <td>0.223531</td>\n",
       "      <td>0.610447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC()</td>\n",
       "      <td>0.559211</td>\n",
       "      <td>0.262581</td>\n",
       "      <td>0.693219</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.562144</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.586466</td>\n",
       "      <td>0.191420</td>\n",
       "      <td>0.607885</td>\n",
       "      <td>0.089744</td>\n",
       "      <td>0.595605</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.720865</td>\n",
       "      <td>0.407968</td>\n",
       "      <td>0.697315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.733083</td>\n",
       "      <td>0.441629</td>\n",
       "      <td>0.725339</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.723078</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.689850</td>\n",
       "      <td>0.274023</td>\n",
       "      <td>0.685671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.644482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.721805   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.726504   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.707707   \n",
       "3                           DecisionTreeClassifier()        0.632519   \n",
       "4                             KNeighborsClassifier()        0.634398   \n",
       "5                                        LinearSVC()        0.559211   \n",
       "6                                       GaussianNB()        0.586466   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.720865   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.733083   \n",
       "9                                    MLPClassifier()        0.689850   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  Warning  \n",
       "0     0.410257   0.698301         0.000000  0.708739        0.000000        0  \n",
       "1     0.417588   0.702693         0.000000  0.712709        0.000000        0  \n",
       "2     0.378500   0.683783         0.000000  0.694102        0.000000        0  \n",
       "3     0.270194   0.639061         0.179487  0.635487        0.202899        0  \n",
       "4     0.223531   0.610447         0.000000  0.620984        0.000000        0  \n",
       "5     0.262581   0.693219         0.078431  0.562144        0.121212        0  \n",
       "6     0.191420   0.607885         0.089744  0.595605        0.129630        0  \n",
       "7     0.407968   0.697315         0.000000  0.707721        0.000000        0  \n",
       "8     0.441629   0.725339         0.500000  0.723078        0.062500        0  \n",
       "9     0.274023   0.685671         0.000000  0.644482        0.000000        0  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove only pH and residual_sugar (weakly correlated with target)\n",
    "\n",
    "#define dfs for x and y\n",
    "y_data = wines_clean['quality_label_cat']\n",
    "x_data2 = wines_clean.drop(['quality_label_cat', 'pH','residual_sugar', 'color_cat', 'quality_label', 'quality', 'color', 'index'], axis = 1) \n",
    "\n",
    "#split training data\n",
    "x_training_data2, x_test_data2, y_training_data2, y_test_data2 = train_test_split(x_data, y_data, test_size = 0.2, random_state=4)\n",
    "\n",
    "deploy_models(x_training_data2, x_test_data2, y_training_data2, y_test_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m#split training data\u001b[39;00m\n\u001b[1;32m      8\u001b[0m x_training_data2, x_test_data2, y_training_data2, y_test_data2 \u001b[39m=\u001b[39m train_test_split(x_data2, y_data, test_size \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m deploy_models(x_training_data2, x_test_data2, y_training_data2, y_test_data2)\n",
      "Cell \u001b[0;32mIn[89], line 24\u001b[0m, in \u001b[0;36mdeploy_models\u001b[0;34m(x_training_data, x_test_data, y_training_data, y_test_data)\u001b[0m\n\u001b[1;32m     22\u001b[0m df_models_eval \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mModel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAccuracy Score\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCohen Kappa\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPrecision\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPrecision: high\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mF1 Score\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mF1 Score: high\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mWarning\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m#make a df to hold the scores\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m classifier_model_defs:             \u001b[39m#loop over models in list\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     model\u001b[39m.\u001b[39;49mfit(x_training_data, y_training_data) \u001b[39m#use fit() to train each model on the x and y training data\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_test_data)           \u001b[39m#predict, evaluate and output some things to compare...\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     acc \u001b[39m=\u001b[39m accuracy_score(y_test_data, pred)\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1291\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1289\u001b[0m     n_threads \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1291\u001b[0m fold_coefs_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, prefer\u001b[39m=\u001b[39;49mprefer)(\n\u001b[1;32m   1292\u001b[0m     path_func(\n\u001b[1;32m   1293\u001b[0m         X,\n\u001b[1;32m   1294\u001b[0m         y,\n\u001b[1;32m   1295\u001b[0m         pos_class\u001b[39m=\u001b[39;49mclass_,\n\u001b[1;32m   1296\u001b[0m         Cs\u001b[39m=\u001b[39;49m[C_],\n\u001b[1;32m   1297\u001b[0m         l1_ratio\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[1;32m   1298\u001b[0m         fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[1;32m   1299\u001b[0m         tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m   1300\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   1301\u001b[0m         solver\u001b[39m=\u001b[39;49msolver,\n\u001b[1;32m   1302\u001b[0m         multi_class\u001b[39m=\u001b[39;49mmulti_class,\n\u001b[1;32m   1303\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m   1304\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m   1305\u001b[0m         check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1306\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[1;32m   1307\u001b[0m         coef\u001b[39m=\u001b[39;49mwarm_start_coef_,\n\u001b[1;32m   1308\u001b[0m         penalty\u001b[39m=\u001b[39;49mpenalty,\n\u001b[1;32m   1309\u001b[0m         max_squared_sum\u001b[39m=\u001b[39;49mmax_squared_sum,\n\u001b[1;32m   1310\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1311\u001b[0m         n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[1;32m   1312\u001b[0m     )\n\u001b[1;32m   1313\u001b[0m     \u001b[39mfor\u001b[39;49;00m class_, warm_start_coef_ \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(classes_, warm_start_coef)\n\u001b[1;32m   1314\u001b[0m )\n\u001b[1;32m   1316\u001b[0m fold_coefs_, _, n_iter_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(n_iter_, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/joblib/parallel.py:1048\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1049\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/joblib/parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 864\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    865\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/joblib/parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    781\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 782\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    783\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:524\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    521\u001b[0m         alpha \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m l1_ratio)\n\u001b[1;32m    522\u001b[0m         beta \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C) \u001b[39m*\u001b[39m l1_ratio\n\u001b[0;32m--> 524\u001b[0m     w0, n_iter_i, warm_start_sag \u001b[39m=\u001b[39m sag_solver(\n\u001b[1;32m    525\u001b[0m         X,\n\u001b[1;32m    526\u001b[0m         target,\n\u001b[1;32m    527\u001b[0m         sample_weight,\n\u001b[1;32m    528\u001b[0m         loss,\n\u001b[1;32m    529\u001b[0m         alpha,\n\u001b[1;32m    530\u001b[0m         beta,\n\u001b[1;32m    531\u001b[0m         max_iter,\n\u001b[1;32m    532\u001b[0m         tol,\n\u001b[1;32m    533\u001b[0m         verbose,\n\u001b[1;32m    534\u001b[0m         random_state,\n\u001b[1;32m    535\u001b[0m         \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    536\u001b[0m         max_squared_sum,\n\u001b[1;32m    537\u001b[0m         warm_start_sag,\n\u001b[1;32m    538\u001b[0m         is_saga\u001b[39m=\u001b[39;49m(solver \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    539\u001b[0m     )\n\u001b[1;32m    541\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msolver must be one of \u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnewton-cg\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39msag\u001b[39m\u001b[39m'\u001b[39m\u001b[39m}, got \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m solver\n\u001b[1;32m    545\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:325\u001b[0m, in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mZeroDivisionError\u001b[39;00m(\n\u001b[1;32m    320\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCurrent sag implementation does not handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe case step_size * alpha_scaled == 1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    324\u001b[0m sag \u001b[39m=\u001b[39m sag64 \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mfloat64 \u001b[39melse\u001b[39;00m sag32\n\u001b[0;32m--> 325\u001b[0m num_seen, n_iter_ \u001b[39m=\u001b[39m sag(\n\u001b[1;32m    326\u001b[0m     dataset,\n\u001b[1;32m    327\u001b[0m     coef_init,\n\u001b[1;32m    328\u001b[0m     intercept_init,\n\u001b[1;32m    329\u001b[0m     n_samples,\n\u001b[1;32m    330\u001b[0m     n_features,\n\u001b[1;32m    331\u001b[0m     n_classes,\n\u001b[1;32m    332\u001b[0m     tol,\n\u001b[1;32m    333\u001b[0m     max_iter,\n\u001b[1;32m    334\u001b[0m     loss,\n\u001b[1;32m    335\u001b[0m     step_size,\n\u001b[1;32m    336\u001b[0m     alpha_scaled,\n\u001b[1;32m    337\u001b[0m     beta_scaled,\n\u001b[1;32m    338\u001b[0m     sum_gradient_init,\n\u001b[1;32m    339\u001b[0m     gradient_memory_init,\n\u001b[1;32m    340\u001b[0m     seen_init,\n\u001b[1;32m    341\u001b[0m     num_seen_init,\n\u001b[1;32m    342\u001b[0m     fit_intercept,\n\u001b[1;32m    343\u001b[0m     intercept_sum_gradient,\n\u001b[1;32m    344\u001b[0m     intercept_decay,\n\u001b[1;32m    345\u001b[0m     is_saga,\n\u001b[1;32m    346\u001b[0m     verbose,\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    349\u001b[0m \u001b[39mif\u001b[39;00m n_iter_ \u001b[39m==\u001b[39m max_iter:\n\u001b[1;32m    350\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    351\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe max_iter was reached which means the coef_ did not converge\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    352\u001b[0m         ConvergenceWarning,\n\u001b[1;32m    353\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#remove only free_sulfur_dioxide, density (strongly correlated to other vars)\n",
    "\n",
    "#define dfs for x and y\n",
    "y_data = wines_clean['quality_label_cat']\n",
    "x_data2 = wines_clean.drop(['quality_label_cat', 'free_sulfur_dioxide', 'density',  'color_cat', 'quality_label', 'quality', 'color', 'index'], axis = 1) \n",
    "\n",
    "#split training data\n",
    "x_training_data2, x_test_data2, y_training_data2, y_test_data2 = train_test_split(x_data2, y_data, test_size = 0.2, random_state=4)\n",
    "\n",
    "deploy_models(x_training_data2, x_test_data2, y_training_data2, y_test_data2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature scaling \n",
    "Note: should be done after splitting to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit scaler on training data eg MinMax and save to a descriptively named variable eg norm/stand..\n",
    "# norm = MinMaxScaler().fit(X_train)\n",
    "\n",
    "## transform training data\n",
    "# X_train_norm = norm.transform(X_train)\n",
    "\n",
    "## transform testing data\n",
    "# X_test_norm = norm.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After normalisation with MinMaxScaler: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "      <th>Warning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.719925</td>\n",
       "      <td>0.404479</td>\n",
       "      <td>0.696164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.716165</td>\n",
       "      <td>0.396486</td>\n",
       "      <td>0.692383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.719925</td>\n",
       "      <td>0.404479</td>\n",
       "      <td>0.696164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.625940</td>\n",
       "      <td>0.258505</td>\n",
       "      <td>0.633823</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.629570</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.694549</td>\n",
       "      <td>0.363111</td>\n",
       "      <td>0.675812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.684900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC()</td>\n",
       "      <td>0.722744</td>\n",
       "      <td>0.410771</td>\n",
       "      <td>0.699036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.572368</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.602291</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.585645</td>\n",
       "      <td>0.115702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.720865</td>\n",
       "      <td>0.407968</td>\n",
       "      <td>0.697315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.744361</td>\n",
       "      <td>0.463628</td>\n",
       "      <td>0.736081</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.733903</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.719925   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.716165   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.719925   \n",
       "3                           DecisionTreeClassifier()        0.625940   \n",
       "4                             KNeighborsClassifier()        0.694549   \n",
       "5                                        LinearSVC()        0.722744   \n",
       "6                                       GaussianNB()        0.572368   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.720865   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.744361   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  Warning  \n",
       "0     0.404479   0.696164         0.000000  0.706373        0.000000        0  \n",
       "1     0.396486   0.692383         0.000000  0.702611        0.000000        0  \n",
       "2     0.404479   0.696164         0.000000  0.706373        0.000000        0  \n",
       "3     0.258505   0.633823         0.142857  0.629570        0.166667        0  \n",
       "4     0.363111   0.675812         0.000000  0.684900        0.000000        0  \n",
       "5     0.410771   0.699036         0.000000  0.709276        0.000000        0  \n",
       "6     0.179100   0.602291         0.076923  0.585645        0.115702        0  \n",
       "7     0.407968   0.697315         0.000000  0.707721        0.000000        0  \n",
       "8     0.463628   0.736081         0.500000  0.733903        0.062500        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Unmodified: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "      <th>Warning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.410257</td>\n",
       "      <td>0.698301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.708739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.726504</td>\n",
       "      <td>0.417588</td>\n",
       "      <td>0.702693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.707707</td>\n",
       "      <td>0.378500</td>\n",
       "      <td>0.683783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.694102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.626880</td>\n",
       "      <td>0.262007</td>\n",
       "      <td>0.634724</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.630243</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.634398</td>\n",
       "      <td>0.223531</td>\n",
       "      <td>0.610447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC()</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.037964</td>\n",
       "      <td>0.679351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.586466</td>\n",
       "      <td>0.191420</td>\n",
       "      <td>0.607885</td>\n",
       "      <td>0.089744</td>\n",
       "      <td>0.595605</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.720865</td>\n",
       "      <td>0.407968</td>\n",
       "      <td>0.697315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.738722</td>\n",
       "      <td>0.454506</td>\n",
       "      <td>0.731179</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.728908</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.721805   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.726504   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.707707   \n",
       "3                           DecisionTreeClassifier()        0.626880   \n",
       "4                             KNeighborsClassifier()        0.634398   \n",
       "5                                        LinearSVC()        0.394737   \n",
       "6                                       GaussianNB()        0.586466   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.720865   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.738722   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  Warning  \n",
       "0     0.410257   0.698301         0.000000  0.708739        0.000000        0  \n",
       "1     0.417588   0.702693         0.000000  0.712709        0.000000        0  \n",
       "2     0.378500   0.683783         0.000000  0.694102        0.000000        0  \n",
       "3     0.262007   0.634724         0.157895  0.630243        0.176471        0  \n",
       "4     0.223531   0.610447         0.000000  0.620984        0.000000        0  \n",
       "5     0.037964   0.679351         0.000000  0.266736        0.000000        0  \n",
       "6     0.191420   0.607885         0.089744  0.595605        0.129630        0  \n",
       "7     0.407968   0.697315         0.000000  0.707721        0.000000        0  \n",
       "8     0.454506   0.731179         0.500000  0.728908        0.062500        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit scaler on training data\n",
    "norm = MinMaxScaler().fit(x_training_data)\n",
    "\n",
    "# transform training data\n",
    "x_training_norm = norm.transform(x_training_data)\n",
    "\n",
    "# transform testing data\n",
    "x_test_norm = norm.transform(x_test_data)\n",
    "\n",
    "# show normalised df\n",
    "#display(pd.DataFrame(x_test_norm))\n",
    "\n",
    "# run these through the testing function\n",
    "norm_results = deploy_models(x_training_norm, x_test_norm, y_training_data, y_test_data)\n",
    "display('After normalisation with MinMaxScaler: ', norm_results)\n",
    "# compare to original\n",
    "display('Unmodified: ', orig_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardisation with RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'After standardisation with RobustScaler: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "      <th>Warning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.724624</td>\n",
       "      <td>0.415354</td>\n",
       "      <td>0.700999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.711317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.413064</td>\n",
       "      <td>0.700018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.724624</td>\n",
       "      <td>0.415354</td>\n",
       "      <td>0.700999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.711317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.258018</td>\n",
       "      <td>0.634456</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.629413</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.693609</td>\n",
       "      <td>0.361452</td>\n",
       "      <td>0.679456</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.685199</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC()</td>\n",
       "      <td>0.722744</td>\n",
       "      <td>0.410771</td>\n",
       "      <td>0.699036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.572368</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.602291</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.585645</td>\n",
       "      <td>0.115702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.720865</td>\n",
       "      <td>0.407968</td>\n",
       "      <td>0.697315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.743421</td>\n",
       "      <td>0.463521</td>\n",
       "      <td>0.735583</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.733391</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.724624   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.723684   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.724624   \n",
       "3                           DecisionTreeClassifier()        0.625000   \n",
       "4                             KNeighborsClassifier()        0.693609   \n",
       "5                                        LinearSVC()        0.722744   \n",
       "6                                       GaussianNB()        0.572368   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.720865   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.743421   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  Warning  \n",
       "0     0.415354   0.700999         0.000000  0.711317        0.000000        0  \n",
       "1     0.413064   0.700018         0.000000  0.710297        0.000000        0  \n",
       "2     0.415354   0.700999         0.000000  0.711317        0.000000        0  \n",
       "3     0.258018   0.634456         0.111111  0.629413        0.133333        0  \n",
       "4     0.361452   0.679456         0.142857  0.685199        0.054054        0  \n",
       "5     0.410771   0.699036         0.000000  0.709276        0.000000        0  \n",
       "6     0.179100   0.602291         0.076923  0.585645        0.115702        0  \n",
       "7     0.407968   0.697315         0.000000  0.707721        0.000000        0  \n",
       "8     0.463521   0.735583         0.500000  0.733391        0.062500        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Unmodified: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Score</th>\n",
       "      <th>Cohen Kappa</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision: high</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F1 Score: high</th>\n",
       "      <th>Warning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.410257</td>\n",
       "      <td>0.698301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.708739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.726504</td>\n",
       "      <td>0.417588</td>\n",
       "      <td>0.702693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.707707</td>\n",
       "      <td>0.378500</td>\n",
       "      <td>0.683783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.694102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.626880</td>\n",
       "      <td>0.262007</td>\n",
       "      <td>0.634724</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.630243</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.634398</td>\n",
       "      <td>0.223531</td>\n",
       "      <td>0.610447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC()</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.037964</td>\n",
       "      <td>0.679351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.586466</td>\n",
       "      <td>0.191420</td>\n",
       "      <td>0.607885</td>\n",
       "      <td>0.089744</td>\n",
       "      <td>0.595605</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.720865</td>\n",
       "      <td>0.407968</td>\n",
       "      <td>0.697315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.738722</td>\n",
       "      <td>0.454506</td>\n",
       "      <td>0.731179</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.728908</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy Score  \\\n",
       "0                  LogisticRegression(max_iter=3000)        0.721805   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...        0.726504   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')        0.707707   \n",
       "3                           DecisionTreeClassifier()        0.626880   \n",
       "4                             KNeighborsClassifier()        0.634398   \n",
       "5                                        LinearSVC()        0.394737   \n",
       "6                                       GaussianNB()        0.586466   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)        0.720865   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...        0.738722   \n",
       "\n",
       "   Cohen Kappa  Precision  Precision: high  F1 Score  F1 Score: high  Warning  \n",
       "0     0.410257   0.698301         0.000000  0.708739        0.000000        0  \n",
       "1     0.417588   0.702693         0.000000  0.712709        0.000000        0  \n",
       "2     0.378500   0.683783         0.000000  0.694102        0.000000        0  \n",
       "3     0.262007   0.634724         0.157895  0.630243        0.176471        0  \n",
       "4     0.223531   0.610447         0.000000  0.620984        0.000000        0  \n",
       "5     0.037964   0.679351         0.000000  0.266736        0.000000        0  \n",
       "6     0.191420   0.607885         0.089744  0.595605        0.129630        0  \n",
       "7     0.407968   0.697315         0.000000  0.707721        0.000000        0  \n",
       "8     0.454506   0.731179         0.500000  0.728908        0.062500        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# StandardScaler: doesn't work correctly in the presence of outliers.\n",
    "# RobustScaler: uses 1st and 3rd quantiles so is better for outliers.\n",
    "\n",
    "# fit scaler on training data\n",
    "robust = RobustScaler().fit(x_training_data)\n",
    "\n",
    "# transform training data\n",
    "x_training_robust = robust.transform(x_training_data)\n",
    "\n",
    "# transform testing data\n",
    "x_test_robust = robust.transform(x_test_data)\n",
    "\n",
    "# run these through the testing function\n",
    "robust_results = deploy_models(x_training_robust, x_test_robust, y_training_data, y_test_data)\n",
    "display('After standardisation with RobustScaler: ', robust_results)\n",
    "# compare to original\n",
    "display('Unmodified: ', orig_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove outliers??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource: https://www.pluralsight.com/guides/cleaning-up-data-from-outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parameter tuning and model improvement "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tuning RandomForest with k-Fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>sulfates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>is_red</th>\n",
       "      <th>is_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.098</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.092</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.075</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.076</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.075</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.039</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.047</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.041</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.022</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.020</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5320 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed_acidity  volatile_acidity  citric_acid  chlorides  \\\n",
       "0               7.8              0.88         0.00      0.098   \n",
       "1               7.8              0.76         0.04      0.092   \n",
       "2              11.2              0.28         0.56      0.075   \n",
       "3               7.4              0.70         0.00      0.076   \n",
       "4               7.4              0.66         0.00      0.075   \n",
       "...             ...               ...          ...        ...   \n",
       "5315            6.2              0.21         0.29      0.039   \n",
       "5316            6.6              0.32         0.36      0.047   \n",
       "5317            6.5              0.24         0.19      0.041   \n",
       "5318            5.5              0.29         0.30      0.022   \n",
       "5319            6.0              0.21         0.38      0.020   \n",
       "\n",
       "      total_sulfur_dioxide  sulfates  alcohol  is_red  is_white  \n",
       "0                     67.0      0.68      9.8     1.0       0.0  \n",
       "1                     54.0      0.65      9.8     1.0       0.0  \n",
       "2                     60.0      0.58      9.8     1.0       0.0  \n",
       "3                     34.0      0.56      9.4     1.0       0.0  \n",
       "4                     40.0      0.56      9.4     1.0       0.0  \n",
       "...                    ...       ...      ...     ...       ...  \n",
       "5315                  92.0      0.50     11.2     0.0       1.0  \n",
       "5316                 168.0      0.46      9.6     0.0       1.0  \n",
       "5317                 111.0      0.46      9.4     0.0       1.0  \n",
       "5318                 110.0      0.38     12.8     0.0       1.0  \n",
       "5319                  98.0      0.32     11.8     0.0       1.0  \n",
       "\n",
       "[5320 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Fold 1</th>\n",
       "      <th>Fold 2</th>\n",
       "      <th>Fold 3</th>\n",
       "      <th>Fold 4</th>\n",
       "      <th>Fold 5</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=3000)</td>\n",
       "      <td>0.713346</td>\n",
       "      <td>0.672932</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.717105</td>\n",
       "      <td>0.766917</td>\n",
       "      <td>0.710902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='libl...</td>\n",
       "      <td>0.707707</td>\n",
       "      <td>0.675752</td>\n",
       "      <td>0.676692</td>\n",
       "      <td>0.722744</td>\n",
       "      <td>0.769737</td>\n",
       "      <td>0.710526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=3000, solver='saga')</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.639098</td>\n",
       "      <td>0.643797</td>\n",
       "      <td>0.711466</td>\n",
       "      <td>0.756579</td>\n",
       "      <td>0.682331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>0.593045</td>\n",
       "      <td>0.580827</td>\n",
       "      <td>0.570489</td>\n",
       "      <td>0.599624</td>\n",
       "      <td>0.641917</td>\n",
       "      <td>0.597180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.527256</td>\n",
       "      <td>0.575188</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>0.640038</td>\n",
       "      <td>0.654135</td>\n",
       "      <td>0.597180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearSVC()</td>\n",
       "      <td>0.658835</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.513158</td>\n",
       "      <td>0.635338</td>\n",
       "      <td>0.690789</td>\n",
       "      <td>0.621053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>0.470865</td>\n",
       "      <td>0.609962</td>\n",
       "      <td>0.563910</td>\n",
       "      <td>0.585526</td>\n",
       "      <td>0.408835</td>\n",
       "      <td>0.527820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=1)</td>\n",
       "      <td>0.718045</td>\n",
       "      <td>0.667293</td>\n",
       "      <td>0.688910</td>\n",
       "      <td>0.719925</td>\n",
       "      <td>0.760338</td>\n",
       "      <td>0.710902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "      <td>0.686090</td>\n",
       "      <td>0.674812</td>\n",
       "      <td>0.704887</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.765038</td>\n",
       "      <td>0.710902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier()</td>\n",
       "      <td>0.688910</td>\n",
       "      <td>0.683271</td>\n",
       "      <td>0.697368</td>\n",
       "      <td>0.664474</td>\n",
       "      <td>0.750940</td>\n",
       "      <td>0.696992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model    Fold 1    Fold 2  \\\n",
       "0                  LogisticRegression(max_iter=3000)  0.713346  0.672932   \n",
       "1  LogisticRegression(max_iter=3000, solver='libl...  0.707707  0.675752   \n",
       "2   LogisticRegression(max_iter=3000, solver='saga')  0.660714  0.639098   \n",
       "3                           DecisionTreeClassifier()  0.593045  0.580827   \n",
       "4                             KNeighborsClassifier()  0.527256  0.575188   \n",
       "5                                        LinearSVC()  0.658835  0.607143   \n",
       "6                                       GaussianNB()  0.470865  0.609962   \n",
       "7         LinearDiscriminantAnalysis(n_components=1)  0.718045  0.667293   \n",
       "8  (DecisionTreeClassifier(max_features='sqrt', r...  0.686090  0.674812   \n",
       "9                                    MLPClassifier()  0.688910  0.683271   \n",
       "\n",
       "     Fold 3    Fold 4    Fold 5  Average Accuracy  \n",
       "0  0.684211  0.717105  0.766917          0.710902  \n",
       "1  0.676692  0.722744  0.769737          0.710526  \n",
       "2  0.643797  0.711466  0.756579          0.682331  \n",
       "3  0.570489  0.599624  0.641917          0.597180  \n",
       "4  0.589286  0.640038  0.654135          0.597180  \n",
       "5  0.513158  0.635338  0.690789          0.621053  \n",
       "6  0.563910  0.585526  0.408835          0.527820  \n",
       "7  0.688910  0.719925  0.760338          0.710902  \n",
       "8  0.704887  0.723684  0.765038          0.710902  \n",
       "9  0.697368  0.664474  0.750940          0.696992  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the scikit-learn method cross_val_score (NB- not the same as cross_validate)\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "#set up a K-Fold with 5 folds\n",
    "kf = KFold(n_splits=5, random_state=None)\n",
    "\n",
    "#make a df to hold the fold scores and average\n",
    "kfold_scores = pd.DataFrame(columns=['Model', 'Fold 1','Fold 2','Fold 3','Fold 4','Fold 5', 'Average Accuracy'])\n",
    "\n",
    "#loop over models, use data with dropped features (x_data1) and RobustScaler for Standardisation- ### is this scaling in the right order, or should it be done after fold split?\n",
    "for model in classifier_model_defs:\n",
    "    result = cross_val_score(model, x_data, y_data, cv = kf)  ##TODO: robust scaler did not work with X_data1, change when fixed above\n",
    "    kfold_scores.loc[len(kfold_scores)] = [model, result[0], result[1], result[2], result[3], result[4], result.mean()]\n",
    "    #print(model, \"Per fold:\".format(result), \"Avg accuracy: {:.4f}\".format(result.mean()))\n",
    "\n",
    "display(kfold_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ideas next:\n",
    "- loop with kfold to evaluate parameters? https://machinelearningmastery.com/random-forest-ensemble-in-python/\n",
    "- function cerating table of average kfold performance per model by scaler? https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf\n",
    "- loop over different datasets with dropped features?\n",
    "- graph model performance?\n",
    "- remove outliers?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
